{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### С другой функцией вознаграждения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import math\n",
    "import random\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyglet\n",
    "import scipy.constants\n",
    "import scipy.integrate\n",
    "from gym import spaces\n",
    "from gym.utils import seeding\n",
    "\n",
    "MIN_VOLTAGE = 0.0\n",
    "MAX_VOLTAGE = 24.0\n",
    "\n",
    "MIN_X = -10.0\n",
    "MAX_X = -MIN_X\n",
    "\n",
    "MIN_XI = -1.0\n",
    "MAX_XI = -MIN_XI\n",
    "\n",
    "MIN_DX = -10.0\n",
    "MAX_DX = -MIN_DX\n",
    "MIN_DXI = MIN_DX\n",
    "MAX_DXI = -MIN_DX\n",
    "\n",
    "\n",
    "class CapsubotEnv(gym.Env):\n",
    "    \"\"\"A cabsubot with electromagnetic coil\"\"\"\n",
    "\n",
    "    metadata = {\"render.modes\": [\"live\", \"file\", \"none\", \"human\"]}\n",
    "    visualization = None\n",
    "\n",
    "    def __init__(self):\n",
    "        super(CapsubotEnv, self).__init__()\n",
    "\n",
    "        # self.is_right_movement = True\n",
    "        self.total_time = None\n",
    "        self.state = None\n",
    "        self.average_speed = 0\n",
    "        self.M = 0.193\n",
    "        self.m = 0.074\n",
    "        self.stiffness = 256.23\n",
    "        self.force_max = 1.25\n",
    "        self.mu = 0.29  # Coefficient of friction.\n",
    "\n",
    "        self.steps_in_period = 200\n",
    "        self.min_period = 0.01\n",
    "        self.dt = self.min_period / self.steps_in_period  # Action force discritization.\n",
    "        self.frame_skip = 40\n",
    "        self.previous_reward = 0.0\n",
    "        self.done = False\n",
    "\n",
    "        self.action_space = spaces.Discrete(2)\n",
    "\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=np.array([MIN_X, MIN_DX, MIN_XI, MIN_DXI]),\n",
    "            high=np.array([MAX_X, MAX_DX, MAX_XI, MAX_DXI]),\n",
    "            dtype=np.float32,\n",
    "        )\n",
    "\n",
    "        self.viewer = None\n",
    "        self.seed()\n",
    "\n",
    "    def seed(self, seed=42):\n",
    "        self.np_random, seed = seeding.np_random(seed)\n",
    "        return [seed]\n",
    "\n",
    "    def F_step(self, action):\n",
    "        return action * self.force_max\n",
    "\n",
    "    def friction_model(self, velocity):\n",
    "        N = (self.M + self.m) * scipy.constants.g\n",
    "        return -N * 2 / np.pi * np.arctan(velocity * 10e5)\n",
    "        # return -np.sign(velocity) * N * self.mu\n",
    "\n",
    "    def mechanical_model(self, y, t, force):\n",
    "        x, x_dot, xi, xi_dot = y\n",
    "        friction = self.friction_model(x_dot)\n",
    "        x_acc = (self.stiffness * xi - force + friction) / self.M\n",
    "        xi_acc = (-self.stiffness * xi + force) / self.m - x_acc\n",
    "        return [x_dot, x_acc, xi_dot, xi_acc]\n",
    "\n",
    "    def step(self, action, integrator=\"euler\"):\n",
    "        err_msg = f\"{action!r} ({type(action)}) invalid\"\n",
    "        assert self.action_space.contains(action), err_msg\n",
    "\n",
    "        for _ in range(self.frame_skip):\n",
    "            x, x_dot, xi, xi_dot = self.state\n",
    "\n",
    "            self.force = self.F_step(action)\n",
    "\n",
    "            # Euler kinematic integration.\n",
    "            dx = self.mechanical_model(self.state, 0, self.force)\n",
    "            self.state = [\n",
    "                x + self.dt * dx[0],\n",
    "                x_dot + self.dt * dx[1],\n",
    "                xi + self.dt * dx[2],\n",
    "                xi_dot + self.dt * dx[3],\n",
    "            ]\n",
    "\n",
    "            self.total_time = self.total_time + self.dt\n",
    "            self.average_speed = x / self.total_time\n",
    "\n",
    "        step_reward = 0\n",
    "        if self.average_speed > 0.0:\n",
    "            reward = self.average_speed * 1000\n",
    "            step_reward = reward - self.previous_reward\n",
    "            self.previous_reward = reward\n",
    "        if x >= 0.05:\n",
    "            self.done = True    # Спросить про вот этот момент\n",
    "            step_reward += 500\n",
    "        elif x <= -0.007:\n",
    "            self.done = True\n",
    "            step_reward -= 1000\n",
    "\n",
    "        return np.array(self.state), step_reward, self.done, {\"average_speed\": self.average_speed}\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = (0, 0, 0, 0)\n",
    "        self.total_time = 0.0\n",
    "        self.average_speed = 0.0\n",
    "        self.previous_reward = 0.0\n",
    "        self.done = False\n",
    "        return np.array(self.state).astype(np.float32)\n",
    "\n",
    "    def render(self, mode=\"human\"):\n",
    "        screen_width = 1280\n",
    "        screen_height = 400\n",
    "\n",
    "        capsule_length = 100.0\n",
    "        capsule_height = 30.0\n",
    "\n",
    "        world_width = 1.0\n",
    "        scale = screen_width / world_width\n",
    "\n",
    "        inner_body_length = capsule_length / 2.0\n",
    "        inner_body_height = capsule_height\n",
    "        inner_body_y = capsule_height\n",
    "\n",
    "        ground_level = 200\n",
    "\n",
    "        if self.viewer is None:\n",
    "            from gym.envs.classic_control import rendering\n",
    "\n",
    "            self.viewer = rendering.Viewer(screen_width, screen_height)\n",
    "            # Capsule polygon.\n",
    "            l, r, t, b = (\n",
    "                -capsule_length / 2,\n",
    "                capsule_length / 2,\n",
    "                capsule_height / 2,\n",
    "                -capsule_height / 2,\n",
    "            )\n",
    "            capsule = rendering.FilledPolygon([(l, b), (l, t), (r, t), (r, b)])\n",
    "            self.capsule_transform = rendering.Transform()\n",
    "            capsule.add_attr(self.capsule_transform)\n",
    "            self.viewer.add_geom(capsule)\n",
    "\n",
    "            # Inner body polygon\n",
    "            l, r, t, b = (\n",
    "                -inner_body_length / 2,\n",
    "                inner_body_length / 2,\n",
    "                inner_body_height / 2,\n",
    "                -inner_body_height / 2,\n",
    "            )\n",
    "            inner_body = rendering.FilledPolygon([(l, b), (l, t), (r, t), (r, b)])\n",
    "            self.inner_body_transform = rendering.Transform()\n",
    "            inner_body.add_attr(self.inner_body_transform)\n",
    "            inner_body.add_attr(self.capsule_transform)\n",
    "            inner_body.set_color(0.8, 0.6, 0.4)\n",
    "            self.viewer.add_geom(inner_body)\n",
    "\n",
    "            # Ground surface\n",
    "            self.track = rendering.Line((0, ground_level), (screen_width, ground_level))\n",
    "            self.track.set_color(0, 0, 0)\n",
    "            self.viewer.add_geom(self.track)\n",
    "\n",
    "            # Score\n",
    "            self.score_label = pyglet.text.Label(\n",
    "                \"0000\",\n",
    "                font_size=36,\n",
    "                x=20,\n",
    "                y=screen_width * 2.5 / 40.00,\n",
    "                anchor_x=\"left\",\n",
    "                anchor_y=\"center\",\n",
    "                color=(255, 255, 255, 255),\n",
    "            )\n",
    "\n",
    "        if self.state is None:\n",
    "            return None\n",
    "\n",
    "        x, x_dot, xi, xi_dot = self.state\n",
    "        capsule_x = x * scale + screen_width / 2.0  # MIDDLE OF CART\n",
    "        capsule_y = ground_level + capsule_height / 2.0  # MIDDLE OF CART\n",
    "        self.capsule_transform.set_translation(capsule_x, capsule_y)\n",
    "\n",
    "        inner_body_x = xi * scale  # MIDDLE OF CART\n",
    "        self.inner_body_transform.set_translation(inner_body_x, inner_body_y)\n",
    "\n",
    "        self.score_label.text = \"%04i\" % self.average_speed\n",
    "        self.score_label.draw()\n",
    "\n",
    "        return self.viewer.render(return_rgb_array=mode == \"rgb_array\")\n",
    "\n",
    "    def close(self):\n",
    "        if self.viewer:\n",
    "            self.viewer.close()\n",
    "            self.viewer = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Test version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import pyglet\n",
    "import scipy.constants\n",
    "import scipy.integrate\n",
    "from gym import spaces\n",
    "from gym.utils import seeding\n",
    "\n",
    "MIN_VOLTAGE = 0.0\n",
    "MAX_VOLTAGE = 24.0\n",
    "\n",
    "MIN_X = -10.0\n",
    "MAX_X = -MIN_X\n",
    "\n",
    "MIN_XI = -1.0\n",
    "MAX_XI = -MIN_XI\n",
    "\n",
    "MIN_DX = -10.0\n",
    "MAX_DX = -MIN_DX\n",
    "MIN_DXI = MIN_DX\n",
    "MAX_DXI = -MIN_DX\n",
    "\n",
    "\n",
    "class CapsubotEnv(gym.Env):\n",
    "    \"\"\"A cabsubot with electromagnetic coil\"\"\"\n",
    "\n",
    "    metadata = {\"render.modes\": [\"live\", \"file\", \"none\", \"human\"]}\n",
    "    visualization = None\n",
    "\n",
    "    def __init__(self):\n",
    "        super(CapsubotEnv, self).__init__()\n",
    "\n",
    "        self.total_time = None\n",
    "        self.state = None\n",
    "        self.average_speed = 0\n",
    "        self.M = 0.193\n",
    "        self.m = 0.074\n",
    "        self.stiffness = 256.23\n",
    "        self.force_max = 1.25\n",
    "        self.mu = 0.29  # Coefficient of friction.\n",
    "\n",
    "        self.steps_in_period = 200\n",
    "        self.min_period = 0.01\n",
    "        self.dt = self.min_period / self.steps_in_period  # Action force discritization.\n",
    "        self.frame_skip = 40\n",
    "        self.previous_average_speed = 0.0\n",
    "        self.done = False\n",
    "\n",
    "        self.action_space = spaces.Discrete(2)\n",
    "\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=np.array([MIN_X, MIN_DX, MIN_XI, MIN_DXI]),\n",
    "            high=np.array([MAX_X, MAX_DX, MAX_XI, MAX_DXI]),\n",
    "            dtype=np.float32,\n",
    "        )\n",
    "\n",
    "        self.viewer = None\n",
    "        self.seed()\n",
    "\n",
    "    def seed(self, seed=42):\n",
    "        self.np_random, seed = seeding.np_random(seed)\n",
    "        return [seed]\n",
    "    \n",
    "    def normalize_state(self, state: list) -> np.ndarray:\n",
    "        state = np.array(state)\n",
    "        norm_state = []\n",
    "        norm_state.append(np.interp(state[0], [-0.0007666844383611218, 0.07919885629789096], [-1, 1]))\n",
    "        norm_state.append(np.interp(state[1], [-0.18283166534706963, 0.24274101317295704], [-1, 1]))\n",
    "        norm_state.append(np.interp(state[2], [-0.01652187660136813, 0.019895362341591866], [-1, 1]))\n",
    "        norm_state.append(np.interp(state[3], [-1.1832780931204638, 1.1508305412787394], [-1, 1]))\n",
    "        return np.array(norm_state)\n",
    "\n",
    "    def F_step(self, action):\n",
    "        return action * self.force_max\n",
    "\n",
    "    def friction_model(self, velocity):\n",
    "        N = (self.M + self.m) * scipy.constants.g\n",
    "        return -N * 2 / np.pi * np.arctan(velocity * 10e5)\n",
    "        # return -np.sign(velocity) * N * self.mu\n",
    "\n",
    "    def mechanical_model(self, y, t, force):\n",
    "        x, x_dot, xi, xi_dot = y\n",
    "        friction = self.friction_model(x_dot)\n",
    "        x_acc = (self.stiffness * xi - force + friction) / self.M\n",
    "        xi_acc = (-self.stiffness * xi + force) / self.m - x_acc\n",
    "        return [x_dot, x_acc, xi_dot, xi_acc]\n",
    "    \n",
    "    def calc_reward(self, av_speed, prev_av_speed, scale_factor=1000) -> float:\n",
    "        return (av_speed  - prev_av_speed) * scale_factor\n",
    "            \n",
    "    def step(self, action):\n",
    "        err_msg = f\"{action!r} ({type(action)}) invalid\"\n",
    "        assert self.action_space.contains(action), err_msg\n",
    "\n",
    "        for _ in range(self.frame_skip):\n",
    "            x, x_dot, xi, xi_dot = self.state\n",
    "            \n",
    "            force = self.F_step(action)\n",
    "\n",
    "            # Euler kinematic integration.\n",
    "            dx = self.mechanical_model(self.state, 0, force)\n",
    "            self.state = [\n",
    "                x + self.dt * dx[0],\n",
    "                x_dot + self.dt * dx[1],\n",
    "                xi + self.dt * dx[2],\n",
    "                xi_dot + self.dt * dx[3],\n",
    "            ]\n",
    "\n",
    "            self.total_time = self.total_time + self.dt\n",
    "            self.average_speed = x / self.total_time\n",
    "        \n",
    "        step_reward = self.calc_reward(self.average_speed, self.previous_average_speed)\n",
    "        norm_state = self.normalize_state(self.state)\n",
    "        # TO DO: normalize reward\n",
    "        self.previous_average_speed = self.average_speed\n",
    "        \n",
    "        if x >= 0.05:\n",
    "            self.done = True  \n",
    "            step_reward += 500\n",
    "        elif x <= -0.007:\n",
    "            self.done = True\n",
    "            step_reward -= 1000\n",
    "\n",
    "        return (\n",
    "            norm_state,\n",
    "            step_reward,\n",
    "            self.done,\n",
    "            {\"average_speed\": self.average_speed},\n",
    "        )\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = (0, 0, 0, 0)\n",
    "        self.total_time = 0.0\n",
    "        self.average_speed = 0.0\n",
    "        self.previous_average_speed = 0.0\n",
    "        self.done = False\n",
    "        return np.array(self.state).astype(np.float32)\n",
    "\n",
    "    def render(self, mode=\"human\"):\n",
    "        screen_width = 1280\n",
    "        screen_height = 400\n",
    "\n",
    "        capsule_length = 100.0\n",
    "        capsule_height = 30.0\n",
    "\n",
    "        world_width = 1.0\n",
    "        scale = screen_width / world_width\n",
    "\n",
    "        inner_body_length = capsule_length / 2.0\n",
    "        inner_body_height = capsule_height\n",
    "        inner_body_y = capsule_height\n",
    "\n",
    "        ground_level = 200\n",
    "\n",
    "        if self.viewer is None:\n",
    "            from gym.envs.classic_control import rendering\n",
    "\n",
    "            self.viewer = rendering.Viewer(screen_width, screen_height)\n",
    "            # Capsule polygon.\n",
    "            l, r, t, b = (\n",
    "                -capsule_length / 2,\n",
    "                capsule_length / 2,\n",
    "                capsule_height / 2,\n",
    "                -capsule_height / 2,\n",
    "            )\n",
    "            capsule = rendering.FilledPolygon([(l, b), (l, t), (r, t), (r, b)])\n",
    "            self.capsule_transform = rendering.Transform()\n",
    "            capsule.add_attr(self.capsule_transform)\n",
    "            self.viewer.add_geom(capsule)\n",
    "\n",
    "            # Inner body polygon\n",
    "            l, r, t, b = (\n",
    "                -inner_body_length / 2,\n",
    "                inner_body_length / 2,\n",
    "                inner_body_height / 2,\n",
    "                -inner_body_height / 2,\n",
    "            )\n",
    "            inner_body = rendering.FilledPolygon([(l, b), (l, t), (r, t), (r, b)])\n",
    "            self.inner_body_transform = rendering.Transform()\n",
    "            inner_body.add_attr(self.inner_body_transform)\n",
    "            inner_body.add_attr(self.capsule_transform)\n",
    "            inner_body.set_color(0.8, 0.6, 0.4)\n",
    "            self.viewer.add_geom(inner_body)\n",
    "\n",
    "            # Ground surface\n",
    "            self.track = rendering.Line((0, ground_level), (screen_width, ground_level))\n",
    "            self.track.set_color(0, 0, 0)\n",
    "            self.viewer.add_geom(self.track)\n",
    "\n",
    "            # Score\n",
    "            self.score_label = pyglet.text.Label(\n",
    "                \"0000\",\n",
    "                font_size=36,\n",
    "                x=20,\n",
    "                y=screen_width * 2.5 / 40.00,\n",
    "                anchor_x=\"left\",\n",
    "                anchor_y=\"center\",\n",
    "                color=(255, 255, 255, 255),\n",
    "            )\n",
    "\n",
    "        if self.state is None:\n",
    "            return None\n",
    "\n",
    "        x, x_dot, xi, xi_dot = self.state\n",
    "        capsule_x = x * scale + screen_width / 2.0  # MIDDLE OF CART\n",
    "        capsule_y = ground_level + capsule_height / 2.0  # MIDDLE OF CART\n",
    "        self.capsule_transform.set_translation(capsule_x, capsule_y)\n",
    "\n",
    "        inner_body_x = xi * scale  # MIDDLE OF CART\n",
    "        self.inner_body_transform.set_translation(inner_body_x, inner_body_y)\n",
    "\n",
    "        self.score_label.text = \"%04i\" % self.average_speed\n",
    "        self.score_label.draw()\n",
    "\n",
    "        return self.viewer.render(return_rgb_array=mode == \"rgb_array\")\n",
    "\n",
    "    def close(self):\n",
    "        if self.viewer:\n",
    "            self.viewer.close()\n",
    "            self.viewer = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import pyglet\n",
    "import scipy.constants\n",
    "import scipy.integrate\n",
    "from gym import spaces\n",
    "from gym.utils import seeding\n",
    "\n",
    "MIN_VOLTAGE = 0.0\n",
    "MAX_VOLTAGE = 24.0\n",
    "\n",
    "MIN_X = -10.0\n",
    "MAX_X = -MIN_X\n",
    "\n",
    "MIN_XI = -1.0\n",
    "MAX_XI = -MIN_XI\n",
    "\n",
    "MIN_DX = -10.0\n",
    "MAX_DX = -MIN_DX\n",
    "MIN_DXI = MIN_DX\n",
    "MAX_DXI = -MIN_DX\n",
    "\n",
    "\n",
    "class CapsubotEnv(gym.Env):\n",
    "    \"\"\"A cabsubot with electromagnetic coil\"\"\"\n",
    "\n",
    "    metadata = {\"render.modes\": [\"live\", \"file\", \"none\", \"human\"]}\n",
    "    visualization = None\n",
    "\n",
    "    def __init__(self):\n",
    "        super(CapsubotEnv, self).__init__()\n",
    "\n",
    "        self.total_time = None\n",
    "        self.state = None\n",
    "        self.M = 0.193\n",
    "        self.m = 0.074\n",
    "        self.stiffness = 256.23\n",
    "        self.force_max = 1.25\n",
    "        self.mu = 0.29  # Coefficient of friction.\n",
    "\n",
    "        self.steps_in_period = 200\n",
    "        self.min_period = 0.01\n",
    "        self.dt = self.min_period / self.steps_in_period  # Action force discritization.\n",
    "        self.frame_skip = 40\n",
    "        self.previous_average_speed = 0.0\n",
    "        self.done = False\n",
    "\n",
    "        self.action_space = spaces.Discrete(2)\n",
    "\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=np.array([MIN_X, MIN_DX, MIN_XI, MIN_DXI]),\n",
    "            high=np.array([MAX_X, MAX_DX, MAX_XI, MAX_DXI]),\n",
    "            dtype=np.float32,\n",
    "        )\n",
    "\n",
    "        self.viewer = None\n",
    "        self.seed()\n",
    "\n",
    "    def seed(self, seed=42):\n",
    "        self.np_random, seed = seeding.np_random(seed)\n",
    "        return [seed]\n",
    "    \n",
    "    def normalize_state(self, state: list) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Normalizing reward states because NN inside RL agent works better with normalized inputs.\n",
    "        Because we can't know the true thresholds of the model states, we use that wierd interpolation.\n",
    "        Thresholds were obtained experimetally.\n",
    "        \"\"\"\n",
    "        state = np.array(state)\n",
    "        norm_state = []\n",
    "        norm_state.append(np.interp(state[0], [-0.0007666844383611218, 0.07919885629789096], [-1, 1]))\n",
    "        norm_state.append(np.interp(state[1], [-0.18283166534706963, 0.24274101317295704], [-1, 1]))\n",
    "        norm_state.append(np.interp(state[2], [-0.01652187660136813, 0.019895362341591866], [-1, 1]))\n",
    "        norm_state.append(np.interp(state[3], [-1.1832780931204638, 1.1508305412787394], [-1, 1]))\n",
    "        return np.array(norm_state)\n",
    "\n",
    "    def F_step(self, action):\n",
    "        return action * self.force_max\n",
    "\n",
    "    def friction_model(self, velocity):\n",
    "        N = (self.M + self.m) * scipy.constants.g\n",
    "        return -N * 2 / np.pi * np.arctan(velocity * 10e5)\n",
    "        # return -np.sign(velocity) * N * self.mu\n",
    "\n",
    "    def mechanical_model(self, y, t, force):\n",
    "        x, x_dot, xi, xi_dot = y\n",
    "        friction = self.friction_model(x_dot)\n",
    "        x_acc = (self.stiffness * xi - force + friction) / self.M\n",
    "        xi_acc = (-self.stiffness * xi + force) / self.m - x_acc\n",
    "        return [x_dot, x_acc, xi_dot, xi_acc]\n",
    "    \n",
    "    def calc_reward(self, av_speed, prev_av_speed, scale_factor=1000) -> float:\n",
    "        return (av_speed  - prev_av_speed) * scale_factor\n",
    "            \n",
    "    def step(self, action):\n",
    "        err_msg = f\"{action!r} ({type(action)}) invalid\"\n",
    "        assert self.action_space.contains(action), err_msg\n",
    "\n",
    "        for _ in range(self.frame_skip):\n",
    "            x, x_dot, xi, xi_dot = self.state\n",
    "            \n",
    "            force = self.F_step(action)\n",
    "\n",
    "            # Euler kinematic integration.\n",
    "            dx = self.mechanical_model(self.state, 0, force)\n",
    "            self.state = [\n",
    "                x + self.dt * dx[0],\n",
    "                x_dot + self.dt * dx[1],\n",
    "                xi + self.dt * dx[2],\n",
    "                xi_dot + self.dt * dx[3],\n",
    "            ]\n",
    "\n",
    "            self.total_time = self.total_time + self.dt\n",
    "            average_speed = x / self.total_time\n",
    "        \n",
    "        step_reward = self.calc_reward(average_speed, self.previous_average_speed)\n",
    "        # TO DO: normalize reward\n",
    "        self.previous_average_speed = average_speed\n",
    "        norm_state = self.normalize_state(self.state)\n",
    "        \n",
    "        if x >= 0.05:\n",
    "            self.done = True  \n",
    "            step_reward += 500\n",
    "        elif x <= -0.007:\n",
    "            self.done = True\n",
    "            step_reward -= 1000\n",
    "\n",
    "        return (\n",
    "            norm_state,\n",
    "            step_reward,\n",
    "            self.done,\n",
    "            {\"average_speed\": average_speed},\n",
    "        )\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = (0, 0, 0, 0)\n",
    "        self.total_time = 0.0\n",
    "        self.previous_average_speed = 0.0\n",
    "        self.done = False\n",
    "        return np.array(self.state).astype(np.float32)\n",
    "\n",
    "    def render(self, mode=\"human\"):\n",
    "        screen_width = 1280\n",
    "        screen_height = 400\n",
    "\n",
    "        capsule_length = 100.0\n",
    "        capsule_height = 30.0\n",
    "\n",
    "        world_width = 1.0\n",
    "        scale = screen_width / world_width\n",
    "\n",
    "        inner_body_length = capsule_length / 2.0\n",
    "        inner_body_height = capsule_height\n",
    "        inner_body_y = capsule_height\n",
    "\n",
    "        ground_level = 200\n",
    "\n",
    "        if self.viewer is None:\n",
    "            from gym.envs.classic_control import rendering\n",
    "\n",
    "            self.viewer = rendering.Viewer(screen_width, screen_height)\n",
    "            # Capsule polygon.\n",
    "            l, r, t, b = (\n",
    "                -capsule_length / 2,\n",
    "                capsule_length / 2,\n",
    "                capsule_height / 2,\n",
    "                -capsule_height / 2,\n",
    "            )\n",
    "            capsule = rendering.FilledPolygon([(l, b), (l, t), (r, t), (r, b)])\n",
    "            self.capsule_transform = rendering.Transform()\n",
    "            capsule.add_attr(self.capsule_transform)\n",
    "            self.viewer.add_geom(capsule)\n",
    "\n",
    "            # Inner body polygon\n",
    "            l, r, t, b = (\n",
    "                -inner_body_length / 2,\n",
    "                inner_body_length / 2,\n",
    "                inner_body_height / 2,\n",
    "                -inner_body_height / 2,\n",
    "            )\n",
    "            inner_body = rendering.FilledPolygon([(l, b), (l, t), (r, t), (r, b)])\n",
    "            self.inner_body_transform = rendering.Transform()\n",
    "            inner_body.add_attr(self.inner_body_transform)\n",
    "            inner_body.add_attr(self.capsule_transform)\n",
    "            inner_body.set_color(0.8, 0.6, 0.4)\n",
    "            self.viewer.add_geom(inner_body)\n",
    "\n",
    "            # Ground surface\n",
    "            self.track = rendering.Line((0, ground_level), (screen_width, ground_level))\n",
    "            self.track.set_color(0, 0, 0)\n",
    "            self.viewer.add_geom(self.track)\n",
    "\n",
    "            # Score\n",
    "            self.score_label = pyglet.text.Label(\n",
    "                \"0000\",\n",
    "                font_size=36,\n",
    "                x=20,\n",
    "                y=screen_width * 2.5 / 40.00,\n",
    "                anchor_x=\"left\",\n",
    "                anchor_y=\"center\",\n",
    "                color=(255, 255, 255, 255),\n",
    "            )\n",
    "\n",
    "        if self.state is None:\n",
    "            return None\n",
    "\n",
    "        x, x_dot, xi, xi_dot = self.state\n",
    "        capsule_x = x * scale + screen_width / 2.0  # MIDDLE OF CART\n",
    "        capsule_y = ground_level + capsule_height / 2.0  # MIDDLE OF CART\n",
    "        self.capsule_transform.set_translation(capsule_x, capsule_y)\n",
    "\n",
    "        inner_body_x = xi * scale  # MIDDLE OF CART\n",
    "        self.inner_body_transform.set_translation(inner_body_x, inner_body_y)\n",
    "\n",
    "        #self.score_label.text = \"%04i\" % self.average_speed\n",
    "        #self.score_label.draw()\n",
    "\n",
    "        return self.viewer.render(return_rgb_array=mode == \"rgb_array\")\n",
    "\n",
    "    def close(self):\n",
    "        if self.viewer:\n",
    "            self.viewer.close()\n",
    "            self.viewer = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import pyglet\n",
    "import scipy.constants\n",
    "import scipy.integrate\n",
    "from gym import spaces\n",
    "from gym.utils import seeding\n",
    "\n",
    "MIN_VOLTAGE = 0.0\n",
    "MAX_VOLTAGE = 24.0\n",
    "\n",
    "MIN_X = -10.0\n",
    "MAX_X = -MIN_X\n",
    "\n",
    "MIN_XI = -1.0\n",
    "MAX_XI = -MIN_XI\n",
    "\n",
    "MIN_DX = -10.0\n",
    "MAX_DX = -MIN_DX\n",
    "MIN_DXI = MIN_DX\n",
    "MAX_DXI = -MIN_DX\n",
    "\n",
    "\n",
    "class CapsubotEnv(gym.Env):\n",
    "    \"\"\"A cabsubot with electromagnetic coil\"\"\"\n",
    "\n",
    "    metadata = {\"render.modes\": [\"live\", \"file\", \"none\", \"human\"]}\n",
    "    visualization = None\n",
    "\n",
    "    def __init__(self):\n",
    "        super(CapsubotEnv, self).__init__()\n",
    "\n",
    "        self.total_time = None\n",
    "        self.state = None\n",
    "        self.average_speed = 0\n",
    "        self.M = 0.193\n",
    "        self.m = 0.074\n",
    "        self.stiffness = 256.23\n",
    "        self.force_max = 1.25\n",
    "        self.mu = 0.29  # Coefficient of friction.\n",
    "\n",
    "        self.steps_in_period = 200\n",
    "        self.min_period = 0.01\n",
    "        self.dt = self.min_period / self.steps_in_period  # Action force discritization.\n",
    "        self.frame_skip = 40 # TODO: frameskip like fraction of min_period or smth\n",
    "        self.previous_average_speed = 0.0\n",
    "        self.done = False\n",
    "\n",
    "        self.action_space = spaces.Discrete(2)\n",
    "\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=np.array([MIN_X, MIN_DX, MIN_XI, MIN_DXI]),\n",
    "            high=np.array([MAX_X, MAX_DX, MAX_XI, MAX_DXI]),\n",
    "            dtype=np.float32,\n",
    "        )\n",
    "\n",
    "        self.viewer = None\n",
    "        self.seed()\n",
    "\n",
    "    def seed(self, seed=42):\n",
    "        self.np_random, seed = seeding.np_random(seed)\n",
    "        return [seed]\n",
    "    \n",
    "    def normalize_state(self, state: list) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Normalizing reward states because NN inside RL agent works better with normalized inputs.\n",
    "        Because we can't know the true thresholds of the model states, we use that wierd interpolation.\n",
    "        Thresholds were obtained experimetally.\n",
    "        \"\"\"\n",
    "        state = np.array(state)\n",
    "        norm_state = []\n",
    "        norm_state.append(np.interp(state[0], [-0.0007666844383611218, 0.07919885629789096], [-1, 1]))\n",
    "        norm_state.append(np.interp(state[1], [-0.18283166534706963, 0.24274101317295704], [-1, 1]))\n",
    "        norm_state.append(np.interp(state[2], [-0.01652187660136813, 0.019895362341591866], [-1, 1]))\n",
    "        norm_state.append(np.interp(state[3], [-1.1832780931204638, 1.1508305412787394], [-1, 1]))\n",
    "        return np.array(norm_state)\n",
    "\n",
    "    def F_step(self, action):\n",
    "        return action * self.force_max\n",
    "\n",
    "    def friction_model(self, velocity):\n",
    "        N = (self.M + self.m) * scipy.constants.g\n",
    "        return -N * 2 / np.pi * np.arctan(velocity * 10e5)\n",
    "        # return -np.sign(velocity) * N * self.mu\n",
    "\n",
    "    def mechanical_model(self, y, t, force):\n",
    "        x, x_dot, xi, xi_dot = y\n",
    "        friction = self.friction_model(x_dot)\n",
    "        x_acc = (self.stiffness * xi - force + friction) / self.M\n",
    "        xi_acc = (-self.stiffness * xi + force) / self.m - x_acc\n",
    "        return [x_dot, x_acc, xi_dot, xi_acc]\n",
    "    \n",
    "    def calc_reward(self, av_speed, prev_av_speed, scale_factor=1000) -> float:\n",
    "        return (av_speed - prev_av_speed) * scale_factor\n",
    "            \n",
    "    def step(self, action, integrator=\"euler\"):\n",
    "        err_msg = f\"{action!r} ({type(action)}) invalid\"\n",
    "        assert self.action_space.contains(action), err_msg\n",
    "\n",
    "        for _ in range(self.frame_skip):\n",
    "            x, x_dot, xi, xi_dot = self.state\n",
    "            \n",
    "            force = self.F_step(action)\n",
    "\n",
    "            # Euler kinematic integration.\n",
    "            dx = self.mechanical_model(self.state, 0, force)\n",
    "            self.state = [\n",
    "                x + self.dt * dx[0],\n",
    "                x_dot + self.dt * dx[1],\n",
    "                xi + self.dt * dx[2],\n",
    "                xi_dot + self.dt * dx[3],\n",
    "            ]\n",
    "\n",
    "            self.total_time = self.total_time + self.dt\n",
    "            self.average_speed = x / self.total_time\n",
    " \n",
    "        norm_state = self.normalize_state(self.state)\n",
    "        # TO DO: normalize reward\n",
    "        step_reward = self.calc_reward(self.average_speed, self.previous_average_speed)\n",
    "        self.previous_average_speed = self.average_speed\n",
    "        \n",
    "        if x >= 0.05:\n",
    "            self.done = True  \n",
    "            step_reward += 500\n",
    "        elif x <= -0.007:\n",
    "            self.done = True\n",
    "            step_reward -= 1000\n",
    "\n",
    "        return (\n",
    "            norm_state,\n",
    "            step_reward,\n",
    "            self.done,\n",
    "            {\"average_speed\": self.average_speed},\n",
    "        )\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = (0, 0, 0, 0)\n",
    "        self.total_time = 0.0\n",
    "        self.average_speed = 0.0\n",
    "        self.previous_average_speed = 0.0\n",
    "        self.done = False\n",
    "        return np.array(self.state).astype(np.float32)\n",
    "\n",
    "    def render(self, mode=\"human\"):\n",
    "        screen_width = 1280\n",
    "        screen_height = 400\n",
    "\n",
    "        capsule_length = 100.0\n",
    "        capsule_height = 30.0\n",
    "\n",
    "        world_width = 1.0\n",
    "        scale = screen_width / world_width\n",
    "\n",
    "        inner_body_length = capsule_length / 2.0\n",
    "        inner_body_height = capsule_height\n",
    "        inner_body_y = capsule_height\n",
    "\n",
    "        ground_level = 200\n",
    "\n",
    "        if self.viewer is None:\n",
    "            from gym.envs.classic_control import rendering\n",
    "\n",
    "            self.viewer = rendering.Viewer(screen_width, screen_height)\n",
    "            # Capsule polygon.\n",
    "            l, r, t, b = (\n",
    "                -capsule_length / 2,\n",
    "                capsule_length / 2,\n",
    "                capsule_height / 2,\n",
    "                -capsule_height / 2,\n",
    "            )\n",
    "            capsule = rendering.FilledPolygon([(l, b), (l, t), (r, t), (r, b)])\n",
    "            self.capsule_transform = rendering.Transform()\n",
    "            capsule.add_attr(self.capsule_transform)\n",
    "            self.viewer.add_geom(capsule)\n",
    "\n",
    "            # Inner body polygon\n",
    "            l, r, t, b = (\n",
    "                -inner_body_length / 2,\n",
    "                inner_body_length / 2,\n",
    "                inner_body_height / 2,\n",
    "                -inner_body_height / 2,\n",
    "            )\n",
    "            inner_body = rendering.FilledPolygon([(l, b), (l, t), (r, t), (r, b)])\n",
    "            self.inner_body_transform = rendering.Transform()\n",
    "            inner_body.add_attr(self.inner_body_transform)\n",
    "            inner_body.add_attr(self.capsule_transform)\n",
    "            inner_body.set_color(0.8, 0.6, 0.4)\n",
    "            self.viewer.add_geom(inner_body)\n",
    "\n",
    "            # Ground surface\n",
    "            self.track = rendering.Line((0, ground_level), (screen_width, ground_level))\n",
    "            self.track.set_color(0, 0, 0)\n",
    "            self.viewer.add_geom(self.track)\n",
    "\n",
    "            # Score\n",
    "            self.score_label = pyglet.text.Label(\n",
    "                \"0000\",\n",
    "                font_size=36,\n",
    "                x=20,\n",
    "                y=screen_width * 2.5 / 40.00,\n",
    "                anchor_x=\"left\",\n",
    "                anchor_y=\"center\",\n",
    "                color=(255, 255, 255, 255),\n",
    "            )\n",
    "\n",
    "        if self.state is None:\n",
    "            return None\n",
    "\n",
    "        x, x_dot, xi, xi_dot = self.state\n",
    "        capsule_x = x * scale + screen_width / 2.0  # MIDDLE OF CART\n",
    "        capsule_y = ground_level + capsule_height / 2.0  # MIDDLE OF CART\n",
    "        self.capsule_transform.set_translation(capsule_x, capsule_y)\n",
    "\n",
    "        inner_body_x = xi * scale  # MIDDLE OF CART\n",
    "        self.inner_body_transform.set_translation(inner_body_x, inner_body_y)\n",
    "\n",
    "        self.score_label.text = \"%04i\" % self.average_speed\n",
    "        self.score_label.draw()\n",
    "\n",
    "        return self.viewer.render(return_rgb_array=mode == \"rgb_array\")\n",
    "\n",
    "    def close(self):\n",
    "        if self.viewer:\n",
    "            self.viewer.close()\n",
    "            self.viewer = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Stable version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import pyglet\n",
    "import scipy.constants\n",
    "import scipy.integrate\n",
    "from gym import spaces\n",
    "from gym.utils import seeding\n",
    "\n",
    "MIN_VOLTAGE = 0.0\n",
    "MAX_VOLTAGE = 24.0\n",
    "\n",
    "MIN_X = -10.0\n",
    "MAX_X = -MIN_X\n",
    "\n",
    "MIN_XI = -1.0\n",
    "MAX_XI = -MIN_XI\n",
    "\n",
    "MIN_DX = -10.0\n",
    "MAX_DX = -MIN_DX\n",
    "MIN_DXI = MIN_DX\n",
    "MAX_DXI = -MIN_DX\n",
    "\n",
    "\n",
    "class CapsubotEnv(gym.Env):\n",
    "    \"\"\"A cabsubot with electromagnetic coil\"\"\"\n",
    "\n",
    "    metadata = {\"render.modes\": [\"live\", \"file\", \"none\", \"human\"]}\n",
    "    visualization = None\n",
    "\n",
    "    def __init__(self):\n",
    "        super(CapsubotEnv, self).__init__()\n",
    "\n",
    "        self.total_time = None\n",
    "        self.state = None\n",
    "        self.average_speed = 0\n",
    "        self.M = 0.193\n",
    "        self.m = 0.074\n",
    "        self.stiffness = 256.23\n",
    "        self.force_max = 1.25\n",
    "        self.mu = 0.29  # Coefficient of friction.\n",
    "\n",
    "        self.steps_in_period = 200\n",
    "        self.min_period = 0.01\n",
    "        self.dt = self.min_period / self.steps_in_period  # Action force discritization.\n",
    "        self.frame_skip = 40 # TODO: frameskip like fraction of min_period or smth\n",
    "        self.previous_reward = 0.0\n",
    "        self.done = False\n",
    "\n",
    "        self.action_space = spaces.Discrete(2)\n",
    "\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=np.array([MIN_X, MIN_DX, MIN_XI, MIN_DXI]),\n",
    "            high=np.array([MAX_X, MAX_DX, MAX_XI, MAX_DXI]),\n",
    "            dtype=np.float32,\n",
    "        )\n",
    "\n",
    "        self.viewer = None\n",
    "        self.seed()\n",
    "\n",
    "    def seed(self, seed=42):\n",
    "        self.np_random, seed = seeding.np_random(seed)\n",
    "        return [seed]\n",
    "    \n",
    "    def normalize_state(self, state: list) -> np.ndarray:\n",
    "        state = np.array(state)\n",
    "        norm_state = []\n",
    "        norm_state.append(np.interp(state[0], [-0.0007666844383611218, 0.07919885629789096], [-1, 1]))\n",
    "        norm_state.append(np.interp(state[1], [-0.18283166534706963, 0.24274101317295704], [-1, 1]))\n",
    "        norm_state.append(np.interp(state[2], [-0.01652187660136813, 0.019895362341591866], [-1, 1]))\n",
    "        norm_state.append(np.interp(state[3], [-1.1832780931204638, 1.1508305412787394], [-1, 1]))\n",
    "        return np.array(norm_state)\n",
    "\n",
    "    def F_step(self, action):\n",
    "        return action * self.force_max\n",
    "\n",
    "    def friction_model(self, velocity):\n",
    "        N = (self.M + self.m) * scipy.constants.g\n",
    "        return -N * 2 / np.pi * np.arctan(velocity * 10e5)\n",
    "        # return -np.sign(velocity) * N * self.mu\n",
    "\n",
    "    def mechanical_model(self, y, t, force):\n",
    "        x, x_dot, xi, xi_dot = y\n",
    "        friction = self.friction_model(x_dot)\n",
    "        x_acc = (self.stiffness * xi - force + friction) / self.M\n",
    "        xi_acc = (-self.stiffness * xi + force) / self.m - x_acc\n",
    "        return [x_dot, x_acc, xi_dot, xi_acc]\n",
    "    \n",
    "    def calc_reward(self, av_speed, prev_reward, x) -> float:\n",
    "        # TODO: normalize reward\n",
    "        step_reward = 0\n",
    "        if av_speed > 0.0:\n",
    "            reward = av_speed * 1000\n",
    "            step_reward = reward - prev_reward\n",
    "            prev_reward = reward\n",
    "        if x >= 0.05:\n",
    "            self.done = True    # Спросить про вот этот момент\n",
    "            step_reward += 500\n",
    "        elif x <= -0.007:\n",
    "            self.done = True\n",
    "            step_reward -= 1000\n",
    "        return step_reward, prev_reward\n",
    "            \n",
    "    def step(self, action, integrator=\"euler\"):\n",
    "        err_msg = f\"{action!r} ({type(action)}) invalid\"\n",
    "        assert self.action_space.contains(action), err_msg\n",
    "\n",
    "        for _ in range(self.frame_skip):\n",
    "            x, x_dot, xi, xi_dot = self.state\n",
    "            \n",
    "            force = self.F_step(action)\n",
    "\n",
    "            # Euler kinematic integration.\n",
    "            dx = self.mechanical_model(self.state, 0, force)\n",
    "            self.state = [\n",
    "                x + self.dt * dx[0],\n",
    "                x_dot + self.dt * dx[1],\n",
    "                xi + self.dt * dx[2],\n",
    "                xi_dot + self.dt * dx[3],\n",
    "            ]\n",
    "\n",
    "            self.total_time = self.total_time + self.dt\n",
    "            self.average_speed = x / self.total_time\n",
    "\n",
    "        step_reward, self.previous_reward = self.calc_reward(self.average_speed, self.previous_reward, x) # Так делать норм?\n",
    "        norm_state = self.normalize_state(self.state)\n",
    "        # TO DO: normalize reward\n",
    "\n",
    "        return (\n",
    "            norm_state,\n",
    "            step_reward,\n",
    "            self.done,\n",
    "            {\"average_speed\": self.average_speed},\n",
    "        )\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = (0, 0, 0, 0)\n",
    "        self.total_time = 0.0\n",
    "        self.average_speed = 0.0\n",
    "        self.previous_reward = 0.0\n",
    "        self.done = False\n",
    "        return np.array(self.state).astype(np.float32)\n",
    "\n",
    "    def render(self, mode=\"human\"):\n",
    "        screen_width = 1280\n",
    "        screen_height = 400\n",
    "\n",
    "        capsule_length = 100.0\n",
    "        capsule_height = 30.0\n",
    "\n",
    "        world_width = 1.0\n",
    "        scale = screen_width / world_width\n",
    "\n",
    "        inner_body_length = capsule_length / 2.0\n",
    "        inner_body_height = capsule_height\n",
    "        inner_body_y = capsule_height\n",
    "\n",
    "        ground_level = 200\n",
    "\n",
    "        if self.viewer is None:\n",
    "            from gym.envs.classic_control import rendering\n",
    "\n",
    "            self.viewer = rendering.Viewer(screen_width, screen_height)\n",
    "            # Capsule polygon.\n",
    "            l, r, t, b = (\n",
    "                -capsule_length / 2,\n",
    "                capsule_length / 2,\n",
    "                capsule_height / 2,\n",
    "                -capsule_height / 2,\n",
    "            )\n",
    "            capsule = rendering.FilledPolygon([(l, b), (l, t), (r, t), (r, b)])\n",
    "            self.capsule_transform = rendering.Transform()\n",
    "            capsule.add_attr(self.capsule_transform)\n",
    "            self.viewer.add_geom(capsule)\n",
    "\n",
    "            # Inner body polygon\n",
    "            l, r, t, b = (\n",
    "                -inner_body_length / 2,\n",
    "                inner_body_length / 2,\n",
    "                inner_body_height / 2,\n",
    "                -inner_body_height / 2,\n",
    "            )\n",
    "            inner_body = rendering.FilledPolygon([(l, b), (l, t), (r, t), (r, b)])\n",
    "            self.inner_body_transform = rendering.Transform()\n",
    "            inner_body.add_attr(self.inner_body_transform)\n",
    "            inner_body.add_attr(self.capsule_transform)\n",
    "            inner_body.set_color(0.8, 0.6, 0.4)\n",
    "            self.viewer.add_geom(inner_body)\n",
    "\n",
    "            # Ground surface\n",
    "            self.track = rendering.Line((0, ground_level), (screen_width, ground_level))\n",
    "            self.track.set_color(0, 0, 0)\n",
    "            self.viewer.add_geom(self.track)\n",
    "\n",
    "            # Score\n",
    "            self.score_label = pyglet.text.Label(\n",
    "                \"0000\",\n",
    "                font_size=36,\n",
    "                x=20,\n",
    "                y=screen_width * 2.5 / 40.00,\n",
    "                anchor_x=\"left\",\n",
    "                anchor_y=\"center\",\n",
    "                color=(255, 255, 255, 255),\n",
    "            )\n",
    "\n",
    "        if self.state is None:\n",
    "            return None\n",
    "\n",
    "        x, x_dot, xi, xi_dot = self.state\n",
    "        capsule_x = x * scale + screen_width / 2.0  # MIDDLE OF CART\n",
    "        capsule_y = ground_level + capsule_height / 2.0  # MIDDLE OF CART\n",
    "        self.capsule_transform.set_translation(capsule_x, capsule_y)\n",
    "\n",
    "        inner_body_x = xi * scale  # MIDDLE OF CART\n",
    "        self.inner_body_transform.set_translation(inner_body_x, inner_body_y)\n",
    "\n",
    "        self.score_label.text = \"%04i\" % self.average_speed\n",
    "        self.score_label.draw()\n",
    "\n",
    "        return self.viewer.render(return_rgb_array=mode == \"rgb_array\")\n",
    "\n",
    "    def close(self):\n",
    "        if self.viewer:\n",
    "            self.viewer.close()\n",
    "            self.viewer = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def action_law(t):\n",
    "    F = 1.25\n",
    "    T = 0.1\n",
    "    tau = 0.3\n",
    "    # return F*F_e(t, T, tau)\n",
    "    return F*F_step(t, T, tau)\n",
    "\n",
    "\n",
    "def F_e(t, T, tau):\n",
    "    \"\"\"\n",
    "    Defines electromagnteic force of coil\n",
    "    \"\"\"\n",
    "    return (1. - 2./np.pi*np.arctan((np.modf(t/T)[0] - tau)*10.E5))/2.\n",
    "\n",
    "\n",
    "def F_step(t, T=0.1, tau=0.7):\n",
    "    part = t/T - t//T\n",
    "    return 1 if part < tau else 0\n",
    "\n",
    "try:\n",
    "    env.close()\n",
    "except Exception:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mi\\anaconda3\\lib\\site-packages\\gym\\spaces\\box.py:73: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# При слишком большом frame_skip график начинает отображаться с искажениями. Потому сначала мы смотрим на график.\n",
    "# По идее, в RL нас не должно это волновать, ведь там мы напрямую передаём управляющий сигнал (1 - сила 1 Н, 0 - сила 0 Н)\n",
    "\n",
    "env = CapsubotEnv()\n",
    "n_periods = 50\n",
    "max_period = 0.1\n",
    "T_max = n_periods*max_period\n",
    "#steps = int(T_max/env.min_period*10) # new model (какой-то бред. НЕ стоит на это смотреть)\n",
    "#steps = int(T_max/env.dt)             # old model\n",
    "steps = int(T_max/env.dt/env.frame_skip)       # end.dt / enf.frame_skip\n",
    "ts = np.linspace(0, T_max, steps)\n",
    "\n",
    "actions = []\n",
    "\n",
    "for t in ts:\n",
    "    action = F_step(t, T = 0.1, tau = 0.3)  # Should move with average velocity\n",
    "    actions.append(action)\n",
    "    \n",
    "plt.plot(ts, actions)\n",
    "plt.xlim(0, 1)\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#int(T_max/env.min_period*10)\n",
    "int(T_max/env.dt/40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "obs_norm = np.interp([-0.002, 0.0000075, 0.15], [-0.01, 0.2], [-1, 1])\n",
    "obs_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_norm = np.interp([-7.48286291e-02, -4.75411857e-07], [-1.151090712002199, 1.183106595060463], [-1, 1])\n",
    "obs_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = CapsubotEnv()\n",
    "n_periods = 50\n",
    "max_period = 0.1\n",
    "T_max = n_periods*max_period\n",
    "#steps = int(T_max/env.min_period*10) # Was T_max/env.dt #FIX IT!!!!!!!!!!!!!!!!!!! (for new model steps = int(T_max/env.min_period*10))\n",
    "#steps = int(T_max/env.dt)             # old model\n",
    "steps = int(T_max/env.dt/env.frame_skip)             # use this model like NEW\n",
    "ts = np.linspace(0, T_max, steps)\n",
    "\n",
    "states = []\n",
    "actions = []\n",
    "rewards = []\n",
    "observ_list = []\n",
    "av_speed_list = []\n",
    "obs = env.reset()\n",
    "for t in ts:\n",
    "    states.append(env.state)\n",
    "    action = F_step(t, T = 0.1, tau = 0.3)  # Should move with average velocity (Просто PWM)\n",
    "    actions.append(action)\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    observ_list.append(obs)\n",
    "    av_speed_list.append(info.get('average_speed'))\n",
    "    #print(f\"env_state {env.state}\")\n",
    "    #print(f\"obs {obs}\")\n",
    "    if done:\n",
    "        print(f\"DONE! x is {env.state[0]} total_tme {env.total_time}\")\n",
    "    rewards.append(reward)\n",
    "    #if (t % 0.005 <= env.dt):\n",
    "    env.render()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Тестим энвайронмент с хардкод весрией ступенчатой силы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of states vector: (2500, 4)\n",
      "length of actions vector: 2500\n",
      "---------------------------\n",
      "last 10 x pos from the states list: [0.06727615 0.06712185 0.06692477 0.06668426 0.06640125 0.06607823\n",
      " 0.06571921 0.06532962 0.06491628 0.06448716]\n",
      "---------------------------\n",
      "rewards: [-0.5973754043467027, -0.7679382211121062, -0.9143433823156286, -1.0307677160041546, -1.112227298978719, -1.1545700089195319, -1.154532915459085, -1.109782806381649, -1.0189393877048558, -0.8815810087556583]\n",
      "---------------------------\n",
      "max reward in list: 500.1505581746236\n",
      "min reward in list: -1.220628371503097\n",
      "---------------------------\n",
      "reward summ: 294012.8124447298\n",
      "---------------------------\n",
      "10 last av_speed elements: [0.013473749161394998, 0.013429018072291281, 0.013375611890541931, 0.013313718917287212, 0.013243840797019063, 0.013166782641597685, 0.013083637353258609, 0.012995764448536387, 0.012904763789986196, 0.01281244472978502]\n"
     ]
    }
   ],
   "source": [
    "states = np.array(states)\n",
    "print(f\"shape of states vector: {states.shape}\")\n",
    "print(f\"length of actions vector: {len(actions)}\")\n",
    "print(\"---------------------------\")\n",
    "print(f\"last 10 x pos from the states list: {states[-10:, 0]}\")\n",
    "print(\"---------------------------\")\n",
    "print(f\"rewards: {rewards[100:110]}\")\n",
    "print(\"---------------------------\")\n",
    "print(f\"max reward in list: {np.amax(rewards)}\")\n",
    "print(f\"min reward in list: {np.amin(rewards)}\")\n",
    "print(\"---------------------------\")\n",
    "print(f\"reward summ: {np.sum(rewards)}\")\n",
    "print(\"---------------------------\")\n",
    "print(f\"10 last av_speed elements: {av_speed_list[-10:]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0675324533158895 \n",
      "\n",
      "-0.00043834639437245244\n"
     ]
    }
   ],
   "source": [
    "print(np.amax(states[:, 0]), \"\\n\")\n",
    "print(np.amin(states[:, 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(states[:,])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Смотрю границы, в которых изменяются элемента observaion, чтобы применить к ним нормалиацию\n",
    "\n",
    "for i in range(states.shape[1]):\n",
    "    print(f\"max: {np.amax(states[:, i])}\")\n",
    "    print(f\"min: {np.amin(states[:, i])}\")\n",
    "    print(\"--------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Тестирую пример нормализации\n",
    "\n",
    "for i in states[2300:2400, 0]:\n",
    "    print(f\"i is {i}\")\n",
    "    obs_norm = np.interp([i], [-0.0007666844383611218, 0.07919885629789096], [-1, 1])\n",
    "    print(obs_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normilize_state(state: list) -> np.ndarray:\n",
    "    norm_state = [None, None, None, None]\n",
    "    norm_state[0] = np.interp(state[0], [-0.0007666844383611218, 0.07919885629789096], [-1, 1])\n",
    "    norm_state[1] = np.interp(state[1], [-0.18283166534706963, 0.24274101317295704], [-1, 1])\n",
    "    norm_state[2] = np.interp(state[2], [-0.01652187660136813, 0.019895362341591866], [-1, 1])\n",
    "    norm_state[3] = np.interp(state[3], [-1.1832780931204638, 1.1508305412787394], [-1, 1])\n",
    "        \n",
    "    return np.array(norm_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normilize_state(state: list) -> np.ndarray:\n",
    "    norm_state = []\n",
    "    norm_state.append(np.interp(state[0], [-0.0007666844383611218, 0.07919885629789096], [-1, 1]))\n",
    "    norm_state.append(np.interp(state[1], [-0.18283166534706963, 0.24274101317295704], [-1, 1]))\n",
    "    norm_state.append(np.interp(state[2], [-0.01652187660136813, 0.019895362341591866], [-1, 1]))\n",
    "    norm_state.append(np.interp(state[3], [-1.1832780931204638, 1.1508305412787394], [-1, 1]))\n",
    "        \n",
    "    return np.array(norm_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_state = normilize_state(states[20])\n",
    "norm_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## На чём я остановился.\n",
    " - Результаты, которые я считал \"хорошими\", оказались наоборот плохохими. Я вместо min_period оставил dt случайно. Таким образом вместо просто 100к таймстепов я получил ещё и по 200 расчётов на каждом из 100к таймстепов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Params before i made some changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# states vector acter changing the env code\n",
    "states_before_change = np.copy(states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states_before_change[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.amax(states_before_change[:, 0]))\n",
    "print(np.amin(states_before_change[:, 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards = np.array(rewards)\n",
    "rewards_before = np.copy(rewards)\n",
    "rewards_before.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = np.array(actions)\n",
    "len(actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Params after i made some changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = np.array(states)\n",
    "states.shape\n",
    "states[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.amax(states[:, 0]))\n",
    "print(np.amin(states[:, 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards = np.array(rewards)\n",
    "rewards.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = np.array(actions)\n",
    "len(actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "\n",
    "models_dir = os.path.join(\n",
    "    \"RL_WIP\", \"RL_data_store\", \"models\", \"PPO-\"\n",
    ") + datetime.datetime.now().strftime(\"%d_%m_%Y-%H_%M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'RL_WIP\\\\RL_data_store\\\\models\\\\PPO-07_09_2022-23_32'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'RL_WIP\\\\RL_data_store\\\\models\\\\PPO-07_09_2022-23_32\\\\asd'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.join(models_dir, \"asd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "TIMESTEPS = 1e5\n",
    "i = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1000000'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\"{int(TIMESTEPS * i)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'RL_WIP\\\\RL_data_store\\\\models\\\\PPO-07_09_2022-23_32\\\\1000000'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.join(models_dir, f\"{int(TIMESTEPS * i)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5e-05\n",
      "200\n"
     ]
    }
   ],
   "source": [
    "steps_in_period = 200\n",
    "min_period = 0.01\n",
    "dt = min_period / steps_in_period  # Action force discritization.\n",
    "frame_skip = steps_in_period\n",
    "print(dt)\n",
    "print(frame_skip)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
