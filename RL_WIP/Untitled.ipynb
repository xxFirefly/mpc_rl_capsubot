{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### С другой функцией вознаграждения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import math\n",
    "import random\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyglet\n",
    "import scipy.constants\n",
    "import scipy.integrate\n",
    "from gym import spaces\n",
    "from gym.utils import seeding\n",
    "\n",
    "MIN_VOLTAGE = 0.0\n",
    "MAX_VOLTAGE = 24.0\n",
    "\n",
    "MIN_X = -10.0\n",
    "MAX_X = -MIN_X\n",
    "\n",
    "MIN_XI = -1.0\n",
    "MAX_XI = -MIN_XI\n",
    "\n",
    "MIN_DX = -10.0\n",
    "MAX_DX = -MIN_DX\n",
    "MIN_DXI = MIN_DX\n",
    "MAX_DXI = -MIN_DX\n",
    "\n",
    "\n",
    "class CapsubotEnv(gym.Env):\n",
    "    \"\"\"A cabsubot with electromagnetic coil\"\"\"\n",
    "\n",
    "    metadata = {\"render.modes\": [\"live\", \"file\", \"none\", \"human\"]}\n",
    "    visualization = None\n",
    "\n",
    "    def __init__(self):\n",
    "        super(CapsubotEnv, self).__init__()\n",
    "\n",
    "        # self.is_right_movement = True\n",
    "        self.total_time = None\n",
    "        self.state = None\n",
    "        self.average_speed = 0\n",
    "        self.M = 0.193\n",
    "        self.m = 0.074\n",
    "        self.stiffness = 256.23\n",
    "        self.force_max = 1.25\n",
    "        self.mu = 0.29  # Coefficient of friction.\n",
    "\n",
    "        self.steps_in_period = 200\n",
    "        self.min_period = 0.01\n",
    "        self.dt = self.min_period / self.steps_in_period  # Action force discritization.\n",
    "        self.frame_skip = 40\n",
    "        self.previous_reward = 0.0\n",
    "        self.done = False\n",
    "\n",
    "        self.action_space = spaces.Discrete(2)\n",
    "\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=np.array([MIN_X, MIN_DX, MIN_XI, MIN_DXI]),\n",
    "            high=np.array([MAX_X, MAX_DX, MAX_XI, MAX_DXI]),\n",
    "            dtype=np.float32,\n",
    "        )\n",
    "\n",
    "        self.viewer = None\n",
    "        self.seed()\n",
    "\n",
    "    def seed(self, seed=42):\n",
    "        self.np_random, seed = seeding.np_random(seed)\n",
    "        return [seed]\n",
    "\n",
    "    def F_step(self, action):\n",
    "        return action * self.force_max\n",
    "\n",
    "    def friction_model(self, velocity):\n",
    "        N = (self.M + self.m) * scipy.constants.g\n",
    "        return -N * 2 / np.pi * np.arctan(velocity * 10e5)\n",
    "        # return -np.sign(velocity) * N * self.mu\n",
    "\n",
    "    def mechanical_model(self, y, t, force):\n",
    "        x, x_dot, xi, xi_dot = y\n",
    "        friction = self.friction_model(x_dot)\n",
    "        x_acc = (self.stiffness * xi - force + friction) / self.M\n",
    "        xi_acc = (-self.stiffness * xi + force) / self.m - x_acc\n",
    "        return [x_dot, x_acc, xi_dot, xi_acc]\n",
    "\n",
    "    def step(self, action, integrator=\"euler\"):\n",
    "        err_msg = f\"{action!r} ({type(action)}) invalid\"\n",
    "        assert self.action_space.contains(action), err_msg\n",
    "\n",
    "        for _ in range(self.frame_skip):\n",
    "            x, x_dot, xi, xi_dot = self.state\n",
    "\n",
    "            self.force = self.F_step(action)\n",
    "\n",
    "            # Euler kinematic integration.\n",
    "            dx = self.mechanical_model(self.state, 0, self.force)\n",
    "            self.state = [\n",
    "                x + self.dt * dx[0],\n",
    "                x_dot + self.dt * dx[1],\n",
    "                xi + self.dt * dx[2],\n",
    "                xi_dot + self.dt * dx[3],\n",
    "            ]\n",
    "\n",
    "            self.total_time = self.total_time + self.dt\n",
    "            self.average_speed = x / self.total_time\n",
    "\n",
    "        step_reward = 0\n",
    "        if self.average_speed > 0.0:\n",
    "            reward = self.average_speed * 1000\n",
    "            step_reward = reward - self.previous_reward\n",
    "            self.previous_reward = reward\n",
    "        if x >= 0.05:\n",
    "            self.done = True    # Спросить про вот этот момент\n",
    "            step_reward += 500\n",
    "        elif x <= -0.007:\n",
    "            self.done = True\n",
    "            step_reward -= 1000\n",
    "\n",
    "        return np.array(self.state), step_reward, self.done, {\"average_speed\": self.average_speed}\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = (0, 0, 0, 0)\n",
    "        self.total_time = 0.0\n",
    "        self.average_speed = 0.0\n",
    "        self.previous_reward = 0.0\n",
    "        self.done = False\n",
    "        return np.array(self.state).astype(np.float32)\n",
    "\n",
    "    def render(self, mode=\"human\"):\n",
    "        screen_width = 1280\n",
    "        screen_height = 400\n",
    "\n",
    "        capsule_length = 100.0\n",
    "        capsule_height = 30.0\n",
    "\n",
    "        world_width = 1.0\n",
    "        scale = screen_width / world_width\n",
    "\n",
    "        inner_body_length = capsule_length / 2.0\n",
    "        inner_body_height = capsule_height\n",
    "        inner_body_y = capsule_height\n",
    "\n",
    "        ground_level = 200\n",
    "\n",
    "        if self.viewer is None:\n",
    "            from gym.envs.classic_control import rendering\n",
    "\n",
    "            self.viewer = rendering.Viewer(screen_width, screen_height)\n",
    "            # Capsule polygon.\n",
    "            l, r, t, b = (\n",
    "                -capsule_length / 2,\n",
    "                capsule_length / 2,\n",
    "                capsule_height / 2,\n",
    "                -capsule_height / 2,\n",
    "            )\n",
    "            capsule = rendering.FilledPolygon([(l, b), (l, t), (r, t), (r, b)])\n",
    "            self.capsule_transform = rendering.Transform()\n",
    "            capsule.add_attr(self.capsule_transform)\n",
    "            self.viewer.add_geom(capsule)\n",
    "\n",
    "            # Inner body polygon\n",
    "            l, r, t, b = (\n",
    "                -inner_body_length / 2,\n",
    "                inner_body_length / 2,\n",
    "                inner_body_height / 2,\n",
    "                -inner_body_height / 2,\n",
    "            )\n",
    "            inner_body = rendering.FilledPolygon([(l, b), (l, t), (r, t), (r, b)])\n",
    "            self.inner_body_transform = rendering.Transform()\n",
    "            inner_body.add_attr(self.inner_body_transform)\n",
    "            inner_body.add_attr(self.capsule_transform)\n",
    "            inner_body.set_color(0.8, 0.6, 0.4)\n",
    "            self.viewer.add_geom(inner_body)\n",
    "\n",
    "            # Ground surface\n",
    "            self.track = rendering.Line((0, ground_level), (screen_width, ground_level))\n",
    "            self.track.set_color(0, 0, 0)\n",
    "            self.viewer.add_geom(self.track)\n",
    "\n",
    "            # Score\n",
    "            self.score_label = pyglet.text.Label(\n",
    "                \"0000\",\n",
    "                font_size=36,\n",
    "                x=20,\n",
    "                y=screen_width * 2.5 / 40.00,\n",
    "                anchor_x=\"left\",\n",
    "                anchor_y=\"center\",\n",
    "                color=(255, 255, 255, 255),\n",
    "            )\n",
    "\n",
    "        if self.state is None:\n",
    "            return None\n",
    "\n",
    "        x, x_dot, xi, xi_dot = self.state\n",
    "        capsule_x = x * scale + screen_width / 2.0  # MIDDLE OF CART\n",
    "        capsule_y = ground_level + capsule_height / 2.0  # MIDDLE OF CART\n",
    "        self.capsule_transform.set_translation(capsule_x, capsule_y)\n",
    "\n",
    "        inner_body_x = xi * scale  # MIDDLE OF CART\n",
    "        self.inner_body_transform.set_translation(inner_body_x, inner_body_y)\n",
    "\n",
    "        self.score_label.text = \"%04i\" % self.average_speed\n",
    "        self.score_label.draw()\n",
    "\n",
    "        return self.viewer.render(return_rgb_array=mode == \"rgb_array\")\n",
    "\n",
    "    def close(self):\n",
    "        if self.viewer:\n",
    "            self.viewer.close()\n",
    "            self.viewer = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Test version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import pyglet\n",
    "import scipy.constants\n",
    "import scipy.integrate\n",
    "from gym import spaces\n",
    "from gym.utils import seeding\n",
    "\n",
    "MIN_VOLTAGE = 0.0\n",
    "MAX_VOLTAGE = 24.0\n",
    "\n",
    "MIN_X = -10.0\n",
    "MAX_X = -MIN_X\n",
    "\n",
    "MIN_XI = -1.0\n",
    "MAX_XI = -MIN_XI\n",
    "\n",
    "MIN_DX = -10.0\n",
    "MAX_DX = -MIN_DX\n",
    "MIN_DXI = MIN_DX\n",
    "MAX_DXI = -MIN_DX\n",
    "\n",
    "\n",
    "class CapsubotEnv(gym.Env):\n",
    "    \"\"\"A cabsubot with electromagnetic coil\"\"\"\n",
    "\n",
    "    metadata = {\"render.modes\": [\"live\", \"file\", \"none\", \"human\"]}\n",
    "    visualization = None\n",
    "\n",
    "    def __init__(self):\n",
    "        super(CapsubotEnv, self).__init__()\n",
    "\n",
    "        self.total_time = None\n",
    "        self.state = None\n",
    "        self.average_speed = 0\n",
    "        self.M = 0.193\n",
    "        self.m = 0.074\n",
    "        self.stiffness = 256.23\n",
    "        self.force_max = 1.25\n",
    "        self.mu = 0.29  # Coefficient of friction.\n",
    "        self.step_counter = 0\n",
    "\n",
    "        self.steps_in_period = 200\n",
    "        self.min_period = 0.01\n",
    "        self.dt = self.min_period / self.steps_in_period  # Action force discritization.\n",
    "        self.frame_skip = self.steps_in_period\n",
    "        # self.frame_skip = 40\n",
    "        self.previous_average_speed = 0.0\n",
    "        self.done = False\n",
    "\n",
    "        self.action_space = spaces.Discrete(2)\n",
    "\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=np.array([MIN_X, MIN_DX, MIN_XI, MIN_DXI]),\n",
    "            high=np.array([MAX_X, MAX_DX, MAX_XI, MAX_DXI]),\n",
    "            dtype=np.float32,\n",
    "        )\n",
    "\n",
    "        self.viewer = None\n",
    "        self.seed()\n",
    "\n",
    "    def seed(self, seed=42):\n",
    "        self.np_random, seed = seeding.np_random(seed)\n",
    "        return [seed]\n",
    "    \n",
    "    def normalize_state(self, state: list) -> np.ndarray:\n",
    "        state = np.array(state)\n",
    "        norm_state = []\n",
    "        norm_state.append(np.interp(state[0], [-0.0007666844383611218, 0.07919885629789096], [-1, 1]))\n",
    "        norm_state.append(np.interp(state[1], [-0.18283166534706963, 0.24274101317295704], [-1, 1]))\n",
    "        norm_state.append(np.interp(state[2], [-0.01652187660136813, 0.019895362341591866], [-1, 1]))\n",
    "        norm_state.append(np.interp(state[3], [-1.1832780931204638, 1.1508305412787394], [-1, 1]))\n",
    "        return np.array(norm_state)\n",
    "\n",
    "    def F_step(self, action):\n",
    "        return action * self.force_max\n",
    "\n",
    "    def friction_model(self, velocity):\n",
    "        N = (self.M + self.m) * scipy.constants.g\n",
    "        return -N * 2 / np.pi * np.arctan(velocity * 10e5)\n",
    "        # return -np.sign(velocity) * N * self.mu\n",
    "\n",
    "    def mechanical_model(self, y, t, force):\n",
    "        x, x_dot, xi, xi_dot = y\n",
    "        friction = self.friction_model(x_dot)\n",
    "        x_acc = (self.stiffness * xi - force + friction) / self.M\n",
    "        xi_acc = (-self.stiffness * xi + force) / self.m - x_acc\n",
    "        return [x_dot, x_acc, xi_dot, xi_acc]\n",
    "    \n",
    "    def calc_reward(self, av_speed, prev_av_speed, scale_factor=1000) -> float:\n",
    "        return (av_speed  - prev_av_speed) * scale_factor\n",
    "            \n",
    "    def step(self, action):\n",
    "        err_msg = f\"{action!r} ({type(action)}) invalid\"\n",
    "        assert self.action_space.contains(action), err_msg\n",
    "        self.step_counter += 1\n",
    "\n",
    "        for _ in range(self.frame_skip):\n",
    "            x, x_dot, xi, xi_dot = self.state\n",
    "            \n",
    "            force = self.F_step(action)\n",
    "\n",
    "            # Euler kinematic integration.\n",
    "            dx = self.mechanical_model(self.state, 0, force)\n",
    "            self.state = [\n",
    "                x + self.dt * dx[0],\n",
    "                x_dot + self.dt * dx[1],\n",
    "                xi + self.dt * dx[2],\n",
    "                xi_dot + self.dt * dx[3],\n",
    "            ]\n",
    "\n",
    "            self.total_time = self.total_time + self.dt\n",
    "            self.average_speed = x / self.total_time\n",
    "        \n",
    "        step_reward = self.calc_reward(self.average_speed, self.previous_average_speed)\n",
    "        norm_state = self.normalize_state(self.state)\n",
    "        # TO DO: normalize reward\n",
    "        self.previous_average_speed = self.average_speed\n",
    "        \n",
    "        if x >= 0.2:\n",
    "            self.done = True  \n",
    "            step_reward += 500\n",
    "        elif x <= -0.007:\n",
    "            self.done = True\n",
    "            step_reward -= 1000\n",
    "\n",
    "        return (\n",
    "            norm_state,\n",
    "            step_reward,\n",
    "            self.done,\n",
    "            {\"average_speed\": self.average_speed},\n",
    "        )\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = (0, 0, 0, 0)\n",
    "        self.total_time = 0.0\n",
    "        self.average_speed = 0.0\n",
    "        self.previous_average_speed = 0.0\n",
    "        self.done = False\n",
    "        self.step_counter = 0\n",
    "        return np.array(self.state).astype(np.float32)\n",
    "\n",
    "    def render(self, mode=\"human\"):\n",
    "        screen_width = 1280\n",
    "        screen_height = 400\n",
    "\n",
    "        capsule_length = 100.0\n",
    "        capsule_height = 30.0\n",
    "\n",
    "        world_width = 1.0\n",
    "        scale = screen_width / world_width\n",
    "\n",
    "        inner_body_length = capsule_length / 2.0\n",
    "        inner_body_height = capsule_height\n",
    "        inner_body_y = capsule_height\n",
    "\n",
    "        ground_level = 200\n",
    "\n",
    "        if self.viewer is None:\n",
    "            from gym.envs.classic_control import rendering\n",
    "\n",
    "            self.viewer = rendering.Viewer(screen_width, screen_height)\n",
    "            # Capsule polygon.\n",
    "            l, r, t, b = (\n",
    "                -capsule_length / 2,\n",
    "                capsule_length / 2,\n",
    "                capsule_height / 2,\n",
    "                -capsule_height / 2,\n",
    "            )\n",
    "            capsule = rendering.FilledPolygon([(l, b), (l, t), (r, t), (r, b)])\n",
    "            self.capsule_transform = rendering.Transform()\n",
    "            capsule.add_attr(self.capsule_transform)\n",
    "            self.viewer.add_geom(capsule)\n",
    "\n",
    "            # Inner body polygon\n",
    "            l, r, t, b = (\n",
    "                -inner_body_length / 2,\n",
    "                inner_body_length / 2,\n",
    "                inner_body_height / 2,\n",
    "                -inner_body_height / 2,\n",
    "            )\n",
    "            inner_body = rendering.FilledPolygon([(l, b), (l, t), (r, t), (r, b)])\n",
    "            self.inner_body_transform = rendering.Transform()\n",
    "            inner_body.add_attr(self.inner_body_transform)\n",
    "            inner_body.add_attr(self.capsule_transform)\n",
    "            inner_body.set_color(0.8, 0.6, 0.4)\n",
    "            self.viewer.add_geom(inner_body)\n",
    "\n",
    "            # Ground surface\n",
    "            self.track = rendering.Line((0, ground_level), (screen_width, ground_level))\n",
    "            self.track.set_color(0, 0, 0)\n",
    "            self.viewer.add_geom(self.track)\n",
    "\n",
    "            # Score\n",
    "            self.score_label = pyglet.text.Label(\n",
    "                \"0000\",\n",
    "                font_size=36,\n",
    "                x=20,\n",
    "                y=screen_width * 2.5 / 40.00,\n",
    "                anchor_x=\"left\",\n",
    "                anchor_y=\"center\",\n",
    "                color=(255, 255, 255, 255),\n",
    "            )\n",
    "\n",
    "        if self.state is None:\n",
    "            return None\n",
    "\n",
    "        x, x_dot, xi, xi_dot = self.state\n",
    "        capsule_x = x * scale + screen_width / 2.0  # MIDDLE OF CART\n",
    "        capsule_y = ground_level + capsule_height / 2.0  # MIDDLE OF CART\n",
    "        self.capsule_transform.set_translation(capsule_x, capsule_y)\n",
    "\n",
    "        inner_body_x = xi * scale  # MIDDLE OF CART\n",
    "        self.inner_body_transform.set_translation(inner_body_x, inner_body_y)\n",
    "\n",
    "        self.score_label.text = \"%04i\" % self.average_speed\n",
    "        self.score_label.draw()\n",
    "\n",
    "        return self.viewer.render(return_rgb_array=mode == \"rgb_array\")\n",
    "\n",
    "    def close(self):\n",
    "        if self.viewer:\n",
    "            self.viewer.close()\n",
    "            self.viewer = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import pyglet\n",
    "import scipy.constants\n",
    "import scipy.integrate\n",
    "from gym import spaces\n",
    "from gym.utils import seeding\n",
    "\n",
    "MIN_VOLTAGE = 0.0\n",
    "MAX_VOLTAGE = 24.0\n",
    "\n",
    "MIN_X = -10.0\n",
    "MAX_X = -MIN_X\n",
    "\n",
    "MIN_XI = -1.0\n",
    "MAX_XI = -MIN_XI\n",
    "\n",
    "MIN_DX = -10.0\n",
    "MAX_DX = -MIN_DX\n",
    "MIN_DXI = MIN_DX\n",
    "MAX_DXI = -MIN_DX\n",
    "\n",
    "\n",
    "class CapsubotEnv(gym.Env):\n",
    "    \"\"\"A cabsubot with electromagnetic coil\"\"\"\n",
    "\n",
    "    metadata = {\"render.modes\": [\"live\", \"file\", \"none\", \"human\"]}\n",
    "    visualization = None\n",
    "\n",
    "    def __init__(self):\n",
    "        super(CapsubotEnv, self).__init__()\n",
    "\n",
    "        self.total_time = None\n",
    "        self.state = None\n",
    "        self.M = 0.193\n",
    "        self.m = 0.074\n",
    "        self.stiffness = 256.23\n",
    "        self.force_max = 1.25\n",
    "        self.mu = 0.29  # Coefficient of friction.\n",
    "\n",
    "        self.steps_in_period = 200\n",
    "        self.min_period = 0.01\n",
    "        self.dt = self.min_period / self.steps_in_period  # Action force discritization.\n",
    "        self.frame_skip = 40\n",
    "        self.previous_average_speed = 0.0\n",
    "        self.done = False\n",
    "\n",
    "        self.action_space = spaces.Discrete(2)\n",
    "\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=np.array([MIN_X, MIN_DX, MIN_XI, MIN_DXI]),\n",
    "            high=np.array([MAX_X, MAX_DX, MAX_XI, MAX_DXI]),\n",
    "            dtype=np.float32,\n",
    "        )\n",
    "\n",
    "        self.viewer = None\n",
    "        self.seed()\n",
    "\n",
    "    def seed(self, seed=42):\n",
    "        self.np_random, seed = seeding.np_random(seed)\n",
    "        return [seed]\n",
    "    \n",
    "    def normalize_state(self, state: list) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Normalizing reward states because NN inside RL agent works better with normalized inputs.\n",
    "        Because we can't know the true thresholds of the model states, we use that wierd interpolation.\n",
    "        Thresholds were obtained experimetally.\n",
    "        \"\"\"\n",
    "        state = np.array(state)\n",
    "        norm_state = []\n",
    "        norm_state.append(np.interp(state[0], [-0.0007666844383611218, 0.07919885629789096], [-1, 1]))\n",
    "        norm_state.append(np.interp(state[1], [-0.18283166534706963, 0.24274101317295704], [-1, 1]))\n",
    "        norm_state.append(np.interp(state[2], [-0.01652187660136813, 0.019895362341591866], [-1, 1]))\n",
    "        norm_state.append(np.interp(state[3], [-1.1832780931204638, 1.1508305412787394], [-1, 1]))\n",
    "        return np.array(norm_state)\n",
    "\n",
    "    def F_step(self, action):\n",
    "        return action * self.force_max\n",
    "\n",
    "    def friction_model(self, velocity):\n",
    "        N = (self.M + self.m) * scipy.constants.g\n",
    "        return -N * 2 / np.pi * np.arctan(velocity * 10e5)\n",
    "        # return -np.sign(velocity) * N * self.mu\n",
    "\n",
    "    def mechanical_model(self, y, t, force):\n",
    "        x, x_dot, xi, xi_dot = y\n",
    "        friction = self.friction_model(x_dot)\n",
    "        x_acc = (self.stiffness * xi - force + friction) / self.M\n",
    "        xi_acc = (-self.stiffness * xi + force) / self.m - x_acc\n",
    "        return [x_dot, x_acc, xi_dot, xi_acc]\n",
    "    \n",
    "    def calc_reward(self, av_speed, prev_av_speed, scale_factor=1000) -> float:\n",
    "        return (av_speed  - prev_av_speed) * scale_factor\n",
    "            \n",
    "    def step(self, action):\n",
    "        err_msg = f\"{action!r} ({type(action)}) invalid\"\n",
    "        assert self.action_space.contains(action), err_msg\n",
    "\n",
    "        for _ in range(self.frame_skip):\n",
    "            x, x_dot, xi, xi_dot = self.state\n",
    "            \n",
    "            force = self.F_step(action)\n",
    "\n",
    "            # Euler kinematic integration.\n",
    "            dx = self.mechanical_model(self.state, 0, force)\n",
    "            self.state = [\n",
    "                x + self.dt * dx[0],\n",
    "                x_dot + self.dt * dx[1],\n",
    "                xi + self.dt * dx[2],\n",
    "                xi_dot + self.dt * dx[3],\n",
    "            ]\n",
    "\n",
    "            self.total_time = self.total_time + self.dt\n",
    "            average_speed = x / self.total_time\n",
    "        \n",
    "        step_reward = self.calc_reward(average_speed, self.previous_average_speed)\n",
    "        # TO DO: normalize reward\n",
    "        self.previous_average_speed = average_speed\n",
    "        norm_state = self.normalize_state(self.state)\n",
    "        \n",
    "        if x >= 0.05:\n",
    "            self.done = True  \n",
    "            step_reward += 500\n",
    "        elif x <= -0.007:\n",
    "            self.done = True\n",
    "            step_reward -= 1000\n",
    "\n",
    "        return (\n",
    "            norm_state,\n",
    "            step_reward,\n",
    "            self.done,\n",
    "            {\"average_speed\": average_speed},\n",
    "        )\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = (0, 0, 0, 0)\n",
    "        self.total_time = 0.0\n",
    "        self.previous_average_speed = 0.0\n",
    "        self.done = False\n",
    "        return np.array(self.state).astype(np.float32)\n",
    "\n",
    "    def render(self, mode=\"human\"):\n",
    "        screen_width = 1280\n",
    "        screen_height = 400\n",
    "\n",
    "        capsule_length = 100.0\n",
    "        capsule_height = 30.0\n",
    "\n",
    "        world_width = 1.0\n",
    "        scale = screen_width / world_width\n",
    "\n",
    "        inner_body_length = capsule_length / 2.0\n",
    "        inner_body_height = capsule_height\n",
    "        inner_body_y = capsule_height\n",
    "\n",
    "        ground_level = 200\n",
    "\n",
    "        if self.viewer is None:\n",
    "            from gym.envs.classic_control import rendering\n",
    "\n",
    "            self.viewer = rendering.Viewer(screen_width, screen_height)\n",
    "            # Capsule polygon.\n",
    "            l, r, t, b = (\n",
    "                -capsule_length / 2,\n",
    "                capsule_length / 2,\n",
    "                capsule_height / 2,\n",
    "                -capsule_height / 2,\n",
    "            )\n",
    "            capsule = rendering.FilledPolygon([(l, b), (l, t), (r, t), (r, b)])\n",
    "            self.capsule_transform = rendering.Transform()\n",
    "            capsule.add_attr(self.capsule_transform)\n",
    "            self.viewer.add_geom(capsule)\n",
    "\n",
    "            # Inner body polygon\n",
    "            l, r, t, b = (\n",
    "                -inner_body_length / 2,\n",
    "                inner_body_length / 2,\n",
    "                inner_body_height / 2,\n",
    "                -inner_body_height / 2,\n",
    "            )\n",
    "            inner_body = rendering.FilledPolygon([(l, b), (l, t), (r, t), (r, b)])\n",
    "            self.inner_body_transform = rendering.Transform()\n",
    "            inner_body.add_attr(self.inner_body_transform)\n",
    "            inner_body.add_attr(self.capsule_transform)\n",
    "            inner_body.set_color(0.8, 0.6, 0.4)\n",
    "            self.viewer.add_geom(inner_body)\n",
    "\n",
    "            # Ground surface\n",
    "            self.track = rendering.Line((0, ground_level), (screen_width, ground_level))\n",
    "            self.track.set_color(0, 0, 0)\n",
    "            self.viewer.add_geom(self.track)\n",
    "\n",
    "            # Score\n",
    "            self.score_label = pyglet.text.Label(\n",
    "                \"0000\",\n",
    "                font_size=36,\n",
    "                x=20,\n",
    "                y=screen_width * 2.5 / 40.00,\n",
    "                anchor_x=\"left\",\n",
    "                anchor_y=\"center\",\n",
    "                color=(255, 255, 255, 255),\n",
    "            )\n",
    "\n",
    "        if self.state is None:\n",
    "            return None\n",
    "\n",
    "        x, x_dot, xi, xi_dot = self.state\n",
    "        capsule_x = x * scale + screen_width / 2.0  # MIDDLE OF CART\n",
    "        capsule_y = ground_level + capsule_height / 2.0  # MIDDLE OF CART\n",
    "        self.capsule_transform.set_translation(capsule_x, capsule_y)\n",
    "\n",
    "        inner_body_x = xi * scale  # MIDDLE OF CART\n",
    "        self.inner_body_transform.set_translation(inner_body_x, inner_body_y)\n",
    "\n",
    "        #self.score_label.text = \"%04i\" % self.average_speed\n",
    "        #self.score_label.draw()\n",
    "\n",
    "        return self.viewer.render(return_rgb_array=mode == \"rgb_array\")\n",
    "\n",
    "    def close(self):\n",
    "        if self.viewer:\n",
    "            self.viewer.close()\n",
    "            self.viewer = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import pyglet\n",
    "import scipy.constants\n",
    "import scipy.integrate\n",
    "from gym import spaces\n",
    "from gym.utils import seeding\n",
    "\n",
    "MIN_VOLTAGE = 0.0\n",
    "MAX_VOLTAGE = 24.0\n",
    "\n",
    "MIN_X = -10.0\n",
    "MAX_X = -MIN_X\n",
    "\n",
    "MIN_XI = -1.0\n",
    "MAX_XI = -MIN_XI\n",
    "\n",
    "MIN_DX = -10.0\n",
    "MAX_DX = -MIN_DX\n",
    "MIN_DXI = MIN_DX\n",
    "MAX_DXI = -MIN_DX\n",
    "\n",
    "\n",
    "class CapsubotEnv(gym.Env):\n",
    "    \"\"\"A cabsubot with electromagnetic coil\"\"\"\n",
    "\n",
    "    metadata = {\"render.modes\": [\"live\", \"file\", \"none\", \"human\"]}\n",
    "    visualization = None\n",
    "\n",
    "    def __init__(self):\n",
    "        super(CapsubotEnv, self).__init__()\n",
    "\n",
    "        self.total_time = None\n",
    "        self.state = None\n",
    "        self.average_speed = 0\n",
    "        self.M = 0.193\n",
    "        self.m = 0.074\n",
    "        self.stiffness = 256.23\n",
    "        self.force_max = 1.25\n",
    "        self.mu = 0.29  # Coefficient of friction.\n",
    "\n",
    "        self.steps_in_period = 200\n",
    "        self.min_period = 0.01\n",
    "        self.dt = self.min_period / self.steps_in_period  # Action force discritization.\n",
    "        self.frame_skip = 40 # TODO: frameskip like fraction of min_period or smth\n",
    "        self.previous_average_speed = 0.0\n",
    "        self.done = False\n",
    "\n",
    "        self.action_space = spaces.Discrete(2)\n",
    "\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=np.array([MIN_X, MIN_DX, MIN_XI, MIN_DXI]),\n",
    "            high=np.array([MAX_X, MAX_DX, MAX_XI, MAX_DXI]),\n",
    "            dtype=np.float32,\n",
    "        )\n",
    "\n",
    "        self.viewer = None\n",
    "        self.seed()\n",
    "\n",
    "    def seed(self, seed=42):\n",
    "        self.np_random, seed = seeding.np_random(seed)\n",
    "        return [seed]\n",
    "    \n",
    "    def normalize_state(self, state: list) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Normalizing reward states because NN inside RL agent works better with normalized inputs.\n",
    "        Because we can't know the true thresholds of the model states, we use that wierd interpolation.\n",
    "        Thresholds were obtained experimetally.\n",
    "        \"\"\"\n",
    "        state = np.array(state)\n",
    "        norm_state = []\n",
    "        norm_state.append(np.interp(state[0], [-0.0007666844383611218, 0.07919885629789096], [-1, 1]))\n",
    "        norm_state.append(np.interp(state[1], [-0.18283166534706963, 0.24274101317295704], [-1, 1]))\n",
    "        norm_state.append(np.interp(state[2], [-0.01652187660136813, 0.019895362341591866], [-1, 1]))\n",
    "        norm_state.append(np.interp(state[3], [-1.1832780931204638, 1.1508305412787394], [-1, 1]))\n",
    "        return np.array(norm_state)\n",
    "\n",
    "    def F_step(self, action):\n",
    "        return action * self.force_max\n",
    "\n",
    "    def friction_model(self, velocity):\n",
    "        N = (self.M + self.m) * scipy.constants.g\n",
    "        return -N * 2 / np.pi * np.arctan(velocity * 10e5)\n",
    "        # return -np.sign(velocity) * N * self.mu\n",
    "\n",
    "    def mechanical_model(self, y, t, force):\n",
    "        x, x_dot, xi, xi_dot = y\n",
    "        friction = self.friction_model(x_dot)\n",
    "        x_acc = (self.stiffness * xi - force + friction) / self.M\n",
    "        xi_acc = (-self.stiffness * xi + force) / self.m - x_acc\n",
    "        return [x_dot, x_acc, xi_dot, xi_acc]\n",
    "    \n",
    "    def calc_reward(self, av_speed, prev_av_speed, scale_factor=1000) -> float:\n",
    "        return (av_speed - prev_av_speed) * scale_factor\n",
    "            \n",
    "    def step(self, action, integrator=\"euler\"):\n",
    "        err_msg = f\"{action!r} ({type(action)}) invalid\"\n",
    "        assert self.action_space.contains(action), err_msg\n",
    "\n",
    "        for _ in range(self.frame_skip):\n",
    "            x, x_dot, xi, xi_dot = self.state\n",
    "            \n",
    "            force = self.F_step(action)\n",
    "\n",
    "            # Euler kinematic integration.\n",
    "            dx = self.mechanical_model(self.state, 0, force)\n",
    "            self.state = [\n",
    "                x + self.dt * dx[0],\n",
    "                x_dot + self.dt * dx[1],\n",
    "                xi + self.dt * dx[2],\n",
    "                xi_dot + self.dt * dx[3],\n",
    "            ]\n",
    "\n",
    "            self.total_time = self.total_time + self.dt\n",
    "            self.average_speed = x / self.total_time\n",
    " \n",
    "        norm_state = self.normalize_state(self.state)\n",
    "        # TO DO: normalize reward\n",
    "        step_reward = self.calc_reward(self.average_speed, self.previous_average_speed)\n",
    "        self.previous_average_speed = self.average_speed\n",
    "        \n",
    "        if x >= 0.05:\n",
    "            self.done = True  \n",
    "            step_reward += 500\n",
    "        elif x <= -0.007:\n",
    "            self.done = True\n",
    "            step_reward -= 1000\n",
    "\n",
    "        return (\n",
    "            norm_state,\n",
    "            step_reward,\n",
    "            self.done,\n",
    "            {\"average_speed\": self.average_speed},\n",
    "        )\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = (0, 0, 0, 0)\n",
    "        self.total_time = 0.0\n",
    "        self.average_speed = 0.0\n",
    "        self.previous_average_speed = 0.0\n",
    "        self.done = False\n",
    "        return np.array(self.state).astype(np.float32)\n",
    "\n",
    "    def render(self, mode=\"human\"):\n",
    "        screen_width = 1280\n",
    "        screen_height = 400\n",
    "\n",
    "        capsule_length = 100.0\n",
    "        capsule_height = 30.0\n",
    "\n",
    "        world_width = 1.0\n",
    "        scale = screen_width / world_width\n",
    "\n",
    "        inner_body_length = capsule_length / 2.0\n",
    "        inner_body_height = capsule_height\n",
    "        inner_body_y = capsule_height\n",
    "\n",
    "        ground_level = 200\n",
    "\n",
    "        if self.viewer is None:\n",
    "            from gym.envs.classic_control import rendering\n",
    "\n",
    "            self.viewer = rendering.Viewer(screen_width, screen_height)\n",
    "            # Capsule polygon.\n",
    "            l, r, t, b = (\n",
    "                -capsule_length / 2,\n",
    "                capsule_length / 2,\n",
    "                capsule_height / 2,\n",
    "                -capsule_height / 2,\n",
    "            )\n",
    "            capsule = rendering.FilledPolygon([(l, b), (l, t), (r, t), (r, b)])\n",
    "            self.capsule_transform = rendering.Transform()\n",
    "            capsule.add_attr(self.capsule_transform)\n",
    "            self.viewer.add_geom(capsule)\n",
    "\n",
    "            # Inner body polygon\n",
    "            l, r, t, b = (\n",
    "                -inner_body_length / 2,\n",
    "                inner_body_length / 2,\n",
    "                inner_body_height / 2,\n",
    "                -inner_body_height / 2,\n",
    "            )\n",
    "            inner_body = rendering.FilledPolygon([(l, b), (l, t), (r, t), (r, b)])\n",
    "            self.inner_body_transform = rendering.Transform()\n",
    "            inner_body.add_attr(self.inner_body_transform)\n",
    "            inner_body.add_attr(self.capsule_transform)\n",
    "            inner_body.set_color(0.8, 0.6, 0.4)\n",
    "            self.viewer.add_geom(inner_body)\n",
    "\n",
    "            # Ground surface\n",
    "            self.track = rendering.Line((0, ground_level), (screen_width, ground_level))\n",
    "            self.track.set_color(0, 0, 0)\n",
    "            self.viewer.add_geom(self.track)\n",
    "\n",
    "            # Score\n",
    "            self.score_label = pyglet.text.Label(\n",
    "                \"0000\",\n",
    "                font_size=36,\n",
    "                x=20,\n",
    "                y=screen_width * 2.5 / 40.00,\n",
    "                anchor_x=\"left\",\n",
    "                anchor_y=\"center\",\n",
    "                color=(255, 255, 255, 255),\n",
    "            )\n",
    "\n",
    "        if self.state is None:\n",
    "            return None\n",
    "\n",
    "        x, x_dot, xi, xi_dot = self.state\n",
    "        capsule_x = x * scale + screen_width / 2.0  # MIDDLE OF CART\n",
    "        capsule_y = ground_level + capsule_height / 2.0  # MIDDLE OF CART\n",
    "        self.capsule_transform.set_translation(capsule_x, capsule_y)\n",
    "\n",
    "        inner_body_x = xi * scale  # MIDDLE OF CART\n",
    "        self.inner_body_transform.set_translation(inner_body_x, inner_body_y)\n",
    "\n",
    "        self.score_label.text = \"%04i\" % self.average_speed\n",
    "        self.score_label.draw()\n",
    "\n",
    "        return self.viewer.render(return_rgb_array=mode == \"rgb_array\")\n",
    "\n",
    "    def close(self):\n",
    "        if self.viewer:\n",
    "            self.viewer.close()\n",
    "            self.viewer = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Stable version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import pyglet\n",
    "import scipy.constants\n",
    "import scipy.integrate\n",
    "from gym import spaces\n",
    "from gym.utils import seeding\n",
    "\n",
    "MIN_VOLTAGE = 0.0\n",
    "MAX_VOLTAGE = 24.0\n",
    "\n",
    "MIN_X = -10.0\n",
    "MAX_X = -MIN_X\n",
    "\n",
    "MIN_XI = -1.0\n",
    "MAX_XI = -MIN_XI\n",
    "\n",
    "MIN_DX = -10.0\n",
    "MAX_DX = -MIN_DX\n",
    "MIN_DXI = MIN_DX\n",
    "MAX_DXI = -MIN_DX\n",
    "\n",
    "\n",
    "class CapsubotEnv(gym.Env):\n",
    "    \"\"\"A cabsubot with electromagnetic coil\"\"\"\n",
    "\n",
    "    metadata = {\"render.modes\": [\"live\", \"file\", \"none\", \"human\"]}\n",
    "    visualization = None\n",
    "\n",
    "    def __init__(self):\n",
    "        super(CapsubotEnv, self).__init__()\n",
    "\n",
    "        self.total_time = None\n",
    "        self.state = None\n",
    "        self.average_speed = 0\n",
    "        self.M = 0.193\n",
    "        self.m = 0.074\n",
    "        self.stiffness = 256.23\n",
    "        self.force_max = 1.25\n",
    "        self.mu = 0.29  # Coefficient of friction.\n",
    "\n",
    "        self.steps_in_period = 200\n",
    "        self.min_period = 0.01\n",
    "        self.dt = self.min_period / self.steps_in_period  # Action force discritization.\n",
    "        self.frame_skip = 40 # TODO: frameskip like fraction of min_period or smth\n",
    "        self.previous_reward = 0.0\n",
    "        self.done = False\n",
    "\n",
    "        self.action_space = spaces.Discrete(2)\n",
    "\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=np.array([MIN_X, MIN_DX, MIN_XI, MIN_DXI]),\n",
    "            high=np.array([MAX_X, MAX_DX, MAX_XI, MAX_DXI]),\n",
    "            dtype=np.float32,\n",
    "        )\n",
    "\n",
    "        self.viewer = None\n",
    "        self.seed()\n",
    "\n",
    "    def seed(self, seed=42):\n",
    "        self.np_random, seed = seeding.np_random(seed)\n",
    "        return [seed]\n",
    "    \n",
    "    def normalize_state(self, state: list) -> np.ndarray:\n",
    "        state = np.array(state)\n",
    "        norm_state = []\n",
    "        norm_state.append(np.interp(state[0], [-0.0007666844383611218, 0.07919885629789096], [-1, 1]))\n",
    "        norm_state.append(np.interp(state[1], [-0.18283166534706963, 0.24274101317295704], [-1, 1]))\n",
    "        norm_state.append(np.interp(state[2], [-0.01652187660136813, 0.019895362341591866], [-1, 1]))\n",
    "        norm_state.append(np.interp(state[3], [-1.1832780931204638, 1.1508305412787394], [-1, 1]))\n",
    "        return np.array(norm_state)\n",
    "\n",
    "    def F_step(self, action):\n",
    "        return action * self.force_max\n",
    "\n",
    "    def friction_model(self, velocity):\n",
    "        N = (self.M + self.m) * scipy.constants.g\n",
    "        return -N * 2 / np.pi * np.arctan(velocity * 10e5)\n",
    "        # return -np.sign(velocity) * N * self.mu\n",
    "\n",
    "    def mechanical_model(self, y, t, force):\n",
    "        x, x_dot, xi, xi_dot = y\n",
    "        friction = self.friction_model(x_dot)\n",
    "        x_acc = (self.stiffness * xi - force + friction) / self.M\n",
    "        xi_acc = (-self.stiffness * xi + force) / self.m - x_acc\n",
    "        return [x_dot, x_acc, xi_dot, xi_acc]\n",
    "    \n",
    "    def calc_reward(self, av_speed, prev_reward, x) -> float:\n",
    "        # TODO: normalize reward\n",
    "        step_reward = 0\n",
    "        if av_speed > 0.0:\n",
    "            reward = av_speed * 1000\n",
    "            step_reward = reward - prev_reward\n",
    "            prev_reward = reward\n",
    "        if x >= 0.05:\n",
    "            self.done = True    # Спросить про вот этот момент\n",
    "            step_reward += 500\n",
    "        elif x <= -0.007:\n",
    "            self.done = True\n",
    "            step_reward -= 1000\n",
    "        return step_reward, prev_reward\n",
    "            \n",
    "    def step(self, action, integrator=\"euler\"):\n",
    "        err_msg = f\"{action!r} ({type(action)}) invalid\"\n",
    "        assert self.action_space.contains(action), err_msg\n",
    "\n",
    "        for _ in range(self.frame_skip):\n",
    "            x, x_dot, xi, xi_dot = self.state\n",
    "            \n",
    "            force = self.F_step(action)\n",
    "\n",
    "            # Euler kinematic integration.\n",
    "            dx = self.mechanical_model(self.state, 0, force)\n",
    "            self.state = [\n",
    "                x + self.dt * dx[0],\n",
    "                x_dot + self.dt * dx[1],\n",
    "                xi + self.dt * dx[2],\n",
    "                xi_dot + self.dt * dx[3],\n",
    "            ]\n",
    "\n",
    "            self.total_time = self.total_time + self.dt\n",
    "            self.average_speed = x / self.total_time\n",
    "\n",
    "        step_reward, self.previous_reward = self.calc_reward(self.average_speed, self.previous_reward, x) # Так делать норм?\n",
    "        norm_state = self.normalize_state(self.state)\n",
    "        # TO DO: normalize reward\n",
    "\n",
    "        return (\n",
    "            norm_state,\n",
    "            step_reward,\n",
    "            self.done,\n",
    "            {\"average_speed\": self.average_speed},\n",
    "        )\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = (0, 0, 0, 0)\n",
    "        self.total_time = 0.0\n",
    "        self.average_speed = 0.0\n",
    "        self.previous_reward = 0.0\n",
    "        self.done = False\n",
    "        return np.array(self.state).astype(np.float32)\n",
    "\n",
    "    def render(self, mode=\"human\"):\n",
    "        screen_width = 1280\n",
    "        screen_height = 400\n",
    "\n",
    "        capsule_length = 100.0\n",
    "        capsule_height = 30.0\n",
    "\n",
    "        world_width = 1.0\n",
    "        scale = screen_width / world_width\n",
    "\n",
    "        inner_body_length = capsule_length / 2.0\n",
    "        inner_body_height = capsule_height\n",
    "        inner_body_y = capsule_height\n",
    "\n",
    "        ground_level = 200\n",
    "\n",
    "        if self.viewer is None:\n",
    "            from gym.envs.classic_control import rendering\n",
    "\n",
    "            self.viewer = rendering.Viewer(screen_width, screen_height)\n",
    "            # Capsule polygon.\n",
    "            l, r, t, b = (\n",
    "                -capsule_length / 2,\n",
    "                capsule_length / 2,\n",
    "                capsule_height / 2,\n",
    "                -capsule_height / 2,\n",
    "            )\n",
    "            capsule = rendering.FilledPolygon([(l, b), (l, t), (r, t), (r, b)])\n",
    "            self.capsule_transform = rendering.Transform()\n",
    "            capsule.add_attr(self.capsule_transform)\n",
    "            self.viewer.add_geom(capsule)\n",
    "\n",
    "            # Inner body polygon\n",
    "            l, r, t, b = (\n",
    "                -inner_body_length / 2,\n",
    "                inner_body_length / 2,\n",
    "                inner_body_height / 2,\n",
    "                -inner_body_height / 2,\n",
    "            )\n",
    "            inner_body = rendering.FilledPolygon([(l, b), (l, t), (r, t), (r, b)])\n",
    "            self.inner_body_transform = rendering.Transform()\n",
    "            inner_body.add_attr(self.inner_body_transform)\n",
    "            inner_body.add_attr(self.capsule_transform)\n",
    "            inner_body.set_color(0.8, 0.6, 0.4)\n",
    "            self.viewer.add_geom(inner_body)\n",
    "\n",
    "            # Ground surface\n",
    "            self.track = rendering.Line((0, ground_level), (screen_width, ground_level))\n",
    "            self.track.set_color(0, 0, 0)\n",
    "            self.viewer.add_geom(self.track)\n",
    "\n",
    "            # Score\n",
    "            self.score_label = pyglet.text.Label(\n",
    "                \"0000\",\n",
    "                font_size=36,\n",
    "                x=20,\n",
    "                y=screen_width * 2.5 / 40.00,\n",
    "                anchor_x=\"left\",\n",
    "                anchor_y=\"center\",\n",
    "                color=(255, 255, 255, 255),\n",
    "            )\n",
    "\n",
    "        if self.state is None:\n",
    "            return None\n",
    "\n",
    "        x, x_dot, xi, xi_dot = self.state\n",
    "        capsule_x = x * scale + screen_width / 2.0  # MIDDLE OF CART\n",
    "        capsule_y = ground_level + capsule_height / 2.0  # MIDDLE OF CART\n",
    "        self.capsule_transform.set_translation(capsule_x, capsule_y)\n",
    "\n",
    "        inner_body_x = xi * scale  # MIDDLE OF CART\n",
    "        self.inner_body_transform.set_translation(inner_body_x, inner_body_y)\n",
    "\n",
    "        self.score_label.text = \"%04i\" % self.average_speed\n",
    "        self.score_label.draw()\n",
    "\n",
    "        return self.viewer.render(return_rgb_array=mode == \"rgb_array\")\n",
    "\n",
    "    def close(self):\n",
    "        if self.viewer:\n",
    "            self.viewer.close()\n",
    "            self.viewer = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def action_law(t):\n",
    "    F = 1.25\n",
    "    T = 0.1\n",
    "    tau = 0.3\n",
    "    # return F*F_e(t, T, tau)\n",
    "    return F*F_step(t, T, tau)\n",
    "\n",
    "\n",
    "def F_e(t, T, tau):\n",
    "    \"\"\"\n",
    "    Defines electromagnteic force of coil\n",
    "    \"\"\"\n",
    "    return (1. - 2./np.pi*np.arctan((np.modf(t/T)[0] - tau)*10.E5))/2.\n",
    "\n",
    "\n",
    "def F_step(t, T=0.1, tau=0.7):\n",
    "    part = t/T - t//T\n",
    "    return 1 if part < tau else 0\n",
    "\n",
    "try:\n",
    "    env.close()\n",
    "except Exception:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# При слишком большом frame_skip график начинает отображаться с искажениями. Потому сначала мы смотрим на график.\n",
    "# По идее, в RL нас не должно это волновать, ведь там мы напрямую передаём управляющий сигнал (1 - сила 1 Н, 0 - сила 0 Н)\n",
    "\n",
    "env = CapsubotEnv()\n",
    "n_periods = 50\n",
    "max_period = 0.1\n",
    "T_max = n_periods*max_period\n",
    "#steps = int(T_max/env.min_period*10) # new model (какой-то бред. НЕ стоит на это смотреть)\n",
    "#steps = int(T_max/env.dt)             # old model\n",
    "steps = int(T_max/env.dt/env.frame_skip)       # end.dt / enf.frame_skip\n",
    "ts = np.linspace(0, T_max, steps)\n",
    "\n",
    "actions = []\n",
    "\n",
    "for t in ts:\n",
    "    action = F_step(t, T = 0.1, tau = 0.3)  # Should move with average velocity\n",
    "    actions.append(action)\n",
    "    \n",
    "plt.plot(ts, actions)\n",
    "plt.xlim(0, 1)\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#int(T_max/env.min_period*10)\n",
    "int(T_max/env.dt/env.frame_skip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "obs_norm = np.interp([-0.002, 0.0000075, 0.15], [-0.01, 0.2], [-1, 1])\n",
    "obs_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_norm = np.interp([-7.48286291e-02, -4.75411857e-07], [-1.151090712002199, 1.183106595060463], [-1, 1])\n",
    "obs_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mi\\anaconda3\\lib\\site-packages\\pyglet\\image\\codecs\\wic.py:406: UserWarning: [WinError -2147417850] Изменение режима для потока после его установки невозможно\n",
      "  warnings.warn(str(err))\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'CapsubotEnv' object has no attribute 'step_counter'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [17]\u001b[0m, in \u001b[0;36m<cell line: 16>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m#print(f\"env_state {env.state}\")\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m#print(f\"obs {obs}\")\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m done:\n\u001b[1;32m---> 26\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDONE! x is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00menv\u001b[38;5;241m.\u001b[39mstate[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m total_tme \u001b[39m\u001b[38;5;132;01m{\u001b[39;00menv\u001b[38;5;241m.\u001b[39mtotal_time\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Step Num = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00menv\u001b[38;5;241m.\u001b[39mstep_counter\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     27\u001b[0m rewards\u001b[38;5;241m.\u001b[39mappend(reward)\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m#if (t % 0.005 <= env.dt):\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'CapsubotEnv' object has no attribute 'step_counter'"
     ]
    }
   ],
   "source": [
    "env = CapsubotEnv()\n",
    "n_periods = 400\n",
    "max_period = 0.1\n",
    "T_max = n_periods*max_period\n",
    "#steps = int(T_max/env.min_period*10) # Was T_max/env.dt #FIX IT!!!!!!!!!!!!!!!!!!! (for new model steps = int(T_max/env.min_period*10))\n",
    "#steps = int(T_max/env.dt)             # old model\n",
    "steps = int(T_max/env.dt/env.frame_skip)             # use this model like NEW\n",
    "ts = np.linspace(0, T_max, steps)\n",
    "\n",
    "states = []\n",
    "actions = []\n",
    "rewards = []\n",
    "observ_list = []\n",
    "av_speed_list = []\n",
    "obs = env.reset()\n",
    "for t in ts:\n",
    "    states.append(env.state)\n",
    "    action = F_step(t, T = 0.1, tau = 0.3)  # Should move with average velocity (Просто PWM)\n",
    "    actions.append(action)\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    observ_list.append(obs)\n",
    "    av_speed_list.append(info.get('average_speed'))\n",
    "    #print(f\"env_state {env.state}\")\n",
    "    #print(f\"obs {obs}\")\n",
    "    if done:\n",
    "        print(f\"DONE! x is {env.state[0]} total_tme {env.total_time}. Step Num = {env.step_counter}\")\n",
    "    rewards.append(reward)\n",
    "    #if (t % 0.005 <= env.dt):\n",
    "    env.render()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Тестим энвайронмент с хардкод весрией ступенчатой силы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of states vector: (500, 4)\n",
      "length of actions vector: 500\n",
      "---------------------------\n",
      "last 10 x pos from the states list: [0.06287103 0.06065848 0.0594237  0.05952759 0.06068299 0.06321093\n",
      " 0.06604548 0.06752507 0.06727615 0.06607823]\n",
      "---------------------------\n",
      "rewards: [-2.389969750661627, -1.4406517607117082, -0.04416995757001475, 0.9685958592330953, 2.2772340017068045, 2.5622248412741944, 1.2875059737569097, -0.39191722448184835, -1.2600922400115357, -2.0047591204815225]\n",
      "---------------------------\n",
      "max reward in list: 7.704378552861692\n",
      "min reward in list: -5.615114883932861\n",
      "---------------------------\n",
      "reward summ: 12.812444729785023\n",
      "---------------------------\n",
      "10 last av_speed elements: [0.012356106301975305, 0.012078208799108443, 0.012074179717808851, 0.012282048949889263, 0.012766923943356415, 0.013313151036035234, 0.01358627411320193, 0.013509934083722683, 0.013243840797019063, 0.01281244472978502]\n"
     ]
    }
   ],
   "source": [
    "states = np.array(states)\n",
    "print(f\"shape of states vector: {states.shape}\")\n",
    "print(f\"length of actions vector: {len(actions)}\")\n",
    "print(\"---------------------------\")\n",
    "print(f\"last 10 x pos from the states list: {states[-10:, 0]}\")\n",
    "print(\"---------------------------\")\n",
    "print(f\"rewards: {rewards[100:110]}\")\n",
    "print(\"---------------------------\")\n",
    "print(f\"max reward in list: {np.amax(rewards)}\")\n",
    "print(f\"min reward in list: {np.amin(rewards)}\")\n",
    "print(\"---------------------------\")\n",
    "print(f\"reward summ: {np.sum(rewards)}\")\n",
    "print(\"---------------------------\")\n",
    "print(f\"10 last av_speed elements: {av_speed_list[-10:]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0675324533158895 \n",
      "\n",
      "-0.00043834639437245244\n"
     ]
    }
   ],
   "source": [
    "print(np.amax(states[:, 0]), \"\\n\")\n",
    "print(np.amin(states[:, 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(states[:,])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Смотрю границы, в которых изменяются элемента observaion, чтобы применить к ним нормалиацию\n",
    "\n",
    "for i in range(states.shape[1]):\n",
    "    print(f\"max: {np.amax(states[:, i])}\")\n",
    "    print(f\"min: {np.amin(states[:, i])}\")\n",
    "    print(\"--------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Тестирую пример нормализации\n",
    "\n",
    "for i in states[2300:2400, 0]:\n",
    "    print(f\"i is {i}\")\n",
    "    obs_norm = np.interp([i], [-0.0007666844383611218, 0.07919885629789096], [-1, 1])\n",
    "    print(obs_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normilize_state(state: list) -> np.ndarray:\n",
    "    norm_state = [None, None, None, None]\n",
    "    norm_state[0] = np.interp(state[0], [-0.0007666844383611218, 0.07919885629789096], [-1, 1])\n",
    "    norm_state[1] = np.interp(state[1], [-0.18283166534706963, 0.24274101317295704], [-1, 1])\n",
    "    norm_state[2] = np.interp(state[2], [-0.01652187660136813, 0.019895362341591866], [-1, 1])\n",
    "    norm_state[3] = np.interp(state[3], [-1.1832780931204638, 1.1508305412787394], [-1, 1])\n",
    "        \n",
    "    return np.array(norm_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normilize_state(state: list) -> np.ndarray:\n",
    "    norm_state = []\n",
    "    norm_state.append(np.interp(state[0], [-0.0007666844383611218, 0.07919885629789096], [-1, 1]))\n",
    "    norm_state.append(np.interp(state[1], [-0.18283166534706963, 0.24274101317295704], [-1, 1]))\n",
    "    norm_state.append(np.interp(state[2], [-0.01652187660136813, 0.019895362341591866], [-1, 1]))\n",
    "    norm_state.append(np.interp(state[3], [-1.1832780931204638, 1.1508305412787394], [-1, 1]))\n",
    "        \n",
    "    return np.array(norm_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_state = normilize_state(states[20])\n",
    "norm_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## На чём я остановился.\n",
    " - Результаты, которые я считал \"хорошими\", оказались наоборот плохохими. Я вместо min_period оставил dt случайно. Таким образом вместо просто 100к таймстепов я получил ещё и по 200 расчётов на каждом из 100к таймстепов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Params before i made some changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# states vector acter changing the env code\n",
    "states_before_change = np.copy(states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states_before_change[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.amax(states_before_change[:, 0]))\n",
    "print(np.amin(states_before_change[:, 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards = np.array(rewards)\n",
    "rewards_before = np.copy(rewards)\n",
    "rewards_before.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = np.array(actions)\n",
    "len(actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Params after i made some changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = np.array(states)\n",
    "states.shape\n",
    "states[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.amax(states[:, 0]))\n",
    "print(np.amin(states[:, 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards = np.array(rewards)\n",
    "rewards.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = np.array(actions)\n",
    "len(actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "\n",
    "models_dir = os.path.join(\n",
    "    \"RL_WIP\", \"RL_data_store\", \"models\", \"PPO-\"\n",
    ") + datetime.datetime.now().strftime(\"%d_%m_%Y-%H_%M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'RL_WIP\\\\RL_data_store\\\\models\\\\PPO-07_09_2022-23_32'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'RL_WIP\\\\RL_data_store\\\\models\\\\PPO-07_09_2022-23_32\\\\asd'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.join(models_dir, \"asd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "TIMESTEPS = 1e5\n",
    "i = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1000000'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\"{int(TIMESTEPS * i)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'RL_WIP\\\\RL_data_store\\\\models\\\\PPO-07_09_2022-23_32\\\\1000000'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.join(models_dir, f\"{int(TIMESTEPS * i)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5e-05\n",
      "200\n"
     ]
    }
   ],
   "source": [
    "steps_in_period = 200\n",
    "min_period = 0.01\n",
    "dt = min_period / steps_in_period  # Action force discritization.\n",
    "frame_skip = steps_in_period\n",
    "print(dt)\n",
    "print(frame_skip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
